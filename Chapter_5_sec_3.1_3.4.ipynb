{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Lab: Cross-Validation and the Bootstrap\n",
    "\n",
    "In this lab, we explore the resampling techniques covered in chapter 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.1 The Validation Set Approach\n",
    "\n",
    "We explore the use of the validation set approach in order to estimate the\n",
    "test error rates that result from fitting various linear models on the Auto\n",
    "data set.\n",
    "Before we begin, we use the np.random.seed() function in order to set a seed for\n",
    "Pythonâ€™s random number generator, so that the reader of this lab will obtain\n",
    "precisely the same results as those shown below. It is generally a good idea\n",
    "to set a random seed when performing an analysis such as cross-validation\n",
    "that contains an element of randomness, so that the results obtained can\n",
    "be reproduced precisely at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd \n",
    "import math\n",
    "import random\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "Auto = pd.read_csv('data/Auto.csv', header=0, na_values='?')\n",
    "Auto = Auto.dropna().reset_index(drop=True) # drop the observation with NA values and reindex the obs from 0\n",
    "                                            # \"drop=True\" means that you don't want it saved as a column.\n",
    "Auto.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Auto.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0          8         307.0       130.0    3504          12.0    70   \n",
       "1  15.0          8         350.0       165.0    3693          11.5    70   \n",
       "2  18.0          8         318.0       150.0    3436          11.0    70   \n",
       "3  16.0          8         304.0       150.0    3433          12.0    70   \n",
       "4  17.0          8         302.0       140.0    3449          10.5    70   \n",
       "\n",
       "   origin                       name  \n",
       "0       1  chevrolet chevelle malibu  \n",
       "1       1          buick skylark 320  \n",
       "2       1         plymouth satellite  \n",
       "3       1              amc rebel sst  \n",
       "4       1                ford torino  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python and R use different random number generators, so we may see slightly difference results in this chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "train = np.random.choice(Auto.shape[0], 196, replace=False) # create train data ids\n",
    "select = np.in1d(range(Auto.shape[0]), train) # create a boolean array which sets TRUE to train data records\n",
    "# np.in1d(): Test whether each element of a 1-D array is also present in a second array.\n",
    "# Returns a boolean array the same length as ar1 that is True where an element of ar1 is in ar2 and False otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 81, 165, 351, 119, 379, 236,  78,  92,  80, 333, 278, 307, 283,\n",
       "       218, 366,   4, 385, 324,   6, 167, 146, 132, 120, 228,   5, 290,\n",
       "       214, 197, 162, 338, 260, 232,  67, 383, 224, 185, 161, 250, 377,\n",
       "        18, 229,  62, 122, 125, 106, 160, 102, 371, 189,  93,  65, 251,\n",
       "       311, 389, 329, 172, 304,  17, 306, 246, 381, 180, 343, 247, 192,\n",
       "       174, 207,  11, 291,  41, 318, 289, 213, 315,  23, 293,  13,  90,\n",
       "        61, 334, 258, 139, 310, 349,  95, 358, 327, 294, 127, 191,  82,\n",
       "       361, 222,  27,  89, 305,  73, 274, 257, 287, 107, 204,  98, 233,\n",
       "       117, 277, 360, 244,  39, 159, 301, 355, 345,  59,  12, 303, 163,\n",
       "        91, 332, 391, 388,  29, 273, 292,  85,  58, 354, 188, 171, 348,\n",
       "       369, 298,  88, 131, 124, 230,  14, 271, 123, 138, 111,  51, 112,\n",
       "         9, 175,  16, 173,   0, 105, 179, 201,  70,  38, 150, 359, 375,\n",
       "       372, 145,  42, 227, 223, 208, 186, 386, 285, 272, 147, 326, 100,\n",
       "        34, 110, 234, 135, 368, 154,  19, 248, 158, 267,  44, 331, 225,\n",
       "       238, 108, 339, 169,  79, 373,  84,   8, 390,  99, 342, 323, 314,\n",
       "        28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   4,   5,   6,   8,   9,  11,  12,  13,  14,  16,  17,  18,\n",
       "        19,  23,  27,  28,  29,  34,  38,  39,  41,  42,  44,  51,  58,\n",
       "        59,  61,  62,  65,  67,  70,  73,  78,  79,  80,  81,  82,  84,\n",
       "        85,  88,  89,  90,  91,  92,  93,  95,  98,  99, 100, 102, 105,\n",
       "       106, 107, 108, 110, 111, 112, 117, 119, 120, 122, 123, 124, 125,\n",
       "       127, 131, 132, 135, 138, 139, 145, 146, 147, 150, 154, 158, 159,\n",
       "       160, 161, 162, 163, 165, 167, 169, 171, 172, 173, 174, 175, 179,\n",
       "       180, 185, 186, 188, 189, 191, 192, 197, 201, 204, 207, 208, 213,\n",
       "       214, 218, 222, 223, 224, 225, 227, 228, 229, 230, 232, 233, 234,\n",
       "       236, 238, 244, 246, 247, 248, 250, 251, 257, 258, 260, 267, 271,\n",
       "       272, 273, 274, 277, 278, 283, 285, 287, 289, 290, 291, 292, 293,\n",
       "       294, 298, 301, 303, 304, 305, 306, 307, 310, 311, 314, 315, 318,\n",
       "       323, 324, 326, 327, 329, 331, 332, 333, 334, 338, 339, 342, 343,\n",
       "       345, 348, 349, 351, 354, 355, 358, 359, 360, 361, 366, 368, 369,\n",
       "       371, 372, 373, 375, 377, 379, 381, 383, 385, 386, 388, 389, 390,\n",
       "       391])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sort()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False,  True,  True,  True, False,  True,\n",
       "        True, False,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True, False, False, False,  True, False, False, False,\n",
       "        True,  True,  True, False, False, False, False,  True, False,\n",
       "       False, False,  True,  True, False,  True,  True, False,  True,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False,  True,  True, False,  True,  True,\n",
       "       False, False,  True, False,  True, False, False,  True, False,\n",
       "       False,  True, False, False, False, False,  True,  True,  True,\n",
       "        True,  True, False,  True,  True, False, False,  True,  True,\n",
       "        True,  True,  True,  True, False,  True, False, False,  True,\n",
       "        True,  True, False,  True, False, False,  True,  True,  True,\n",
       "        True, False,  True,  True,  True, False, False, False, False,\n",
       "        True, False,  True,  True, False,  True,  True,  True,  True,\n",
       "       False,  True, False, False, False,  True,  True, False, False,\n",
       "        True, False, False,  True,  True, False, False, False, False,\n",
       "       False,  True,  True,  True, False, False,  True, False, False,\n",
       "       False,  True, False, False, False,  True,  True,  True,  True,\n",
       "        True,  True, False,  True, False,  True, False,  True, False,\n",
       "        True,  True,  True,  True,  True, False, False, False,  True,\n",
       "        True, False, False, False, False,  True,  True, False,  True,\n",
       "        True, False,  True,  True, False, False, False, False,  True,\n",
       "       False, False, False,  True, False, False,  True, False, False,\n",
       "        True,  True, False, False, False, False,  True,  True, False,\n",
       "       False, False,  True, False, False, False,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True, False,  True,  True,\n",
       "        True, False,  True, False,  True, False, False, False, False,\n",
       "       False,  True, False,  True,  True,  True, False,  True,  True,\n",
       "       False, False, False, False, False,  True,  True, False,  True,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False,  True,  True,  True,  True, False, False,  True,  True,\n",
       "       False, False, False, False,  True, False,  True, False,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True, False, False,\n",
       "       False,  True, False, False,  True, False,  True,  True,  True,\n",
       "        True,  True, False, False,  True,  True, False, False,  True,\n",
       "        True, False, False,  True, False, False, False, False,  True,\n",
       "        True, False,  True,  True, False,  True, False,  True,  True,\n",
       "        True,  True, False, False, False,  True,  True, False, False,\n",
       "        True,  True, False,  True, False, False,  True,  True, False,\n",
       "        True, False, False,  True,  True, False, False,  True,  True,\n",
       "        True,  True, False, False, False, False,  True, False,  True,\n",
       "        True, False,  True,  True,  True, False,  True, False,  True,\n",
       "       False,  True, False,  True, False,  True, False,  True,  True,\n",
       "       False,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392,)\n"
     ]
    }
   ],
   "source": [
    "print(select.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.620\n",
      "Model:                            OLS   Adj. R-squared:                  0.618\n",
      "Method:                 Least Squares   F-statistic:                     316.4\n",
      "Date:                Thu, 13 Sep 2018   Prob (F-statistic):           1.28e-42\n",
      "Time:                        18:15:22   Log-Likelihood:                -592.07\n",
      "No. Observations:                 196   AIC:                             1188.\n",
      "Df Residuals:                     194   BIC:                             1195.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     40.3338      1.023     39.416      0.000      38.316      42.352\n",
      "horsepower    -0.1596      0.009    -17.788      0.000      -0.177      -0.142\n",
      "==============================================================================\n",
      "Omnibus:                        8.393   Durbin-Watson:                   1.061\n",
      "Prob(Omnibus):                  0.015   Jarque-Bera (JB):                8.787\n",
      "Skew:                           0.516   Prob(JB):                       0.0124\n",
      "Kurtosis:                       2.899   Cond. No.                         328.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "--------Test Error for 1st order--------\n",
      "23.361902892587235\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "lm = smf.ols ('mpg~horsepower', data = Auto[select]).fit()\n",
    "print (lm.summary())\n",
    "preds = lm.predict(Auto)\n",
    "square_error = (Auto['mpg'] - preds)**2\n",
    "print ('--------Test Error for 1st order--------')\n",
    "print (np.mean(square_error[~select]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Test Error for 2nd order--------\n",
      "20.25269085835017\n"
     ]
    }
   ],
   "source": [
    "lm2 = smf.ols ('mpg~horsepower + I(horsepower ** 2.0)', data = Auto[select]).fit()\n",
    "preds = lm2.predict(Auto)\n",
    "square_error = (Auto['mpg'] - preds)**2\n",
    "print ('--------Test Error for 2nd order--------')\n",
    "print (np.mean(square_error[~select]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Test Error for 3rd order--------\n",
      "20.325609365878517\n"
     ]
    }
   ],
   "source": [
    "lm3 = smf.ols ('mpg~horsepower + I(horsepower ** 2.0) + I(horsepower ** 3.0)', data = Auto[select]).fit()\n",
    "preds = lm3.predict(Auto)\n",
    "square_error = (Auto['mpg'] - preds)**2\n",
    "print ('--------Test Error for 3rd order--------')\n",
    "print (np.mean(square_error[~select]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These results are consistent with our previous findings: a model that predicts mpg using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower, and there is little evidence in favor of a model that uses a cubic function of horsepower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we look at the summmary for 3rd order regression, the coefficient of the 3rd order term is not statistically significant. We can use this as Supporting evidence for the above claim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.722\n",
      "Model:                            OLS   Adj. R-squared:                  0.717\n",
      "Method:                 Least Squares   F-statistic:                     165.9\n",
      "Date:                Thu, 13 Sep 2018   Prob (F-statistic):           4.60e-53\n",
      "Time:                        18:23:00   Log-Likelihood:                -561.56\n",
      "No. Observations:                 196   AIC:                             1131.\n",
      "Df Residuals:                     192   BIC:                             1144.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept               66.5200      6.310     10.541      0.000      54.073      78.967\n",
      "horsepower              -0.6868      0.162     -4.238      0.000      -1.006      -0.367\n",
      "I(horsepower ** 2.0)     0.0028      0.001      2.157      0.032       0.000       0.005\n",
      "I(horsepower ** 3.0) -3.524e-06   3.27e-06     -1.078      0.282   -9.97e-06    2.92e-06\n",
      "==============================================================================\n",
      "Omnibus:                        9.054   Durbin-Watson:                   1.328\n",
      "Prob(Omnibus):                  0.011   Jarque-Bera (JB):               15.936\n",
      "Skew:                           0.174   Prob(JB):                     0.000346\n",
      "Kurtosis:                       4.353   Cond. No.                     5.83e+07\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.83e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "print (lm3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying CV in Python is not as easy as that in R. It will require some manual coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use some of implemented functions in Python, we use Sklearn for linear model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.93586102117047\n",
      "[-0.15784473]\n"
     ]
    }
   ],
   "source": [
    "x = pd.DataFrame(Auto.horsepower)\n",
    "y = Auto.mpg\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "print (model.intercept_)\n",
    "print (model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x), type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Folds cross-validator \n",
    "provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default): http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross_val_score evaluates a score by cross-validation\n",
    "cv: for integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "    \n",
    "scoring: All scorer objects follow the convention that higher return values are better than lower return values. \n",
    "Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error, \n",
    "are available as neg_mean_squared_error which return the negated value of the metric. ( -100 > -1000)\n",
    ", http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    \n",
    "n_jobs: The number of CPUs to use to do the computation. -1 means â€˜all CPUsâ€™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=x.shape[0]) # Loocv use folds equal to # of observations\n",
    "test = cross_val_score(model, x, y, cv=k_fold,  scoring = 'neg_mean_squared_error', n_jobs=-1)\n",
    "print (np.mean(-test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.02001002e+00, -1.25092412e+00, -3.06805164e+00, -6.79901984e-02,\n",
       "       -7.08255629e-01, -4.13566745e+01, -8.13755358e+01, -6.71494767e+01,\n",
       "       -9.70498847e+01, -2.63430368e+01, -3.67428697e+00, -4.70741691e-01,\n",
       "       -1.60507789e+00, -9.70498847e+01, -8.89557151e-01, -8.69418110e+00,\n",
       "       -4.41228965e+01, -3.06562225e+01, -9.16549732e-01, -4.53185378e+01,\n",
       "       -1.45705281e+00, -3.00983554e+00, -3.54617605e-03, -1.52964088e+01,\n",
       "       -2.25022208e+01, -1.67905446e+01, -2.76735351e+00, -1.85354648e+01,\n",
       "       -2.29957474e-01, -9.16549732e-01, -5.18379999e+00, -3.54617605e-03,\n",
       "       -2.66745510e+01, -5.44791120e+01, -5.14078324e+01, -4.99405256e+01,\n",
       "       -3.80360006e+01, -1.19884585e-02, -2.91033003e+00, -3.23104363e+00,\n",
       "       -5.16691118e+00, -2.32487332e-01, -1.06678908e-02, -4.82615264e-01,\n",
       "       -2.10211114e+01, -4.35585197e+01, -2.66745510e+01, -6.51231162e+01,\n",
       "       -1.13690418e+01, -5.18379999e+00, -1.25085727e+00, -4.27873211e+00,\n",
       "       -1.77161819e+00, -3.58044876e+01, -1.21519855e+01, -8.41044041e+00,\n",
       "       -8.89557151e-01, -5.36657259e+00, -7.17595842e+01, -3.30230842e+01,\n",
       "       -2.89239650e+01, -8.09034686e-01, -2.91033003e+00, -1.60507789e+00,\n",
       "       -3.23104363e+00, -5.55648116e-01, -1.58374562e+01, -6.18633936e+00,\n",
       "       -7.30069293e+00, -9.62066868e+00, -3.18079355e+01, -1.60507789e+00,\n",
       "       -4.14699861e+01, -2.36253376e+01, -5.16691118e+00, -1.82205869e+01,\n",
       "       -3.55599555e+01, -2.72424889e+01, -9.35761228e+00, -1.91401940e+01,\n",
       "       -6.72450828e+00, -2.65440672e+00, -4.81922115e-01, -9.16549732e-01,\n",
       "       -4.82615264e-01, -5.16691118e+00, -1.65673457e+01, -1.87496691e+01,\n",
       "       -1.60507789e+00, -1.14041800e+01, -1.07534901e+01, -4.04602925e+00,\n",
       "       -5.16691118e+00, -5.14127073e+01, -6.07551884e+01, -4.82615264e-01,\n",
       "       -2.89000924e+01, -6.67900464e+01, -3.80360006e+01, -6.51231162e+01,\n",
       "       -3.78643546e+00, -4.53185378e+01, -2.80008847e+01, -2.52995804e+00,\n",
       "       -1.06678908e-02, -2.32487332e-01, -3.80360006e+01, -3.67700670e+01,\n",
       "       -5.78250435e+01, -9.65326195e+00, -6.00999134e+01, -5.69004321e+01,\n",
       "       -4.20960109e+00, -7.34161330e-02, -4.24141077e+00, -1.62504118e+02,\n",
       "       -1.04124952e+01, -1.69263872e+01, -3.12256930e+01, -1.06661158e+01,\n",
       "       -1.60507789e+00, -2.04714617e+00, -4.63627225e-01, -2.81308530e-01,\n",
       "       -2.45422744e+01, -2.66745510e+01, -8.41826427e+01, -2.71568614e+00,\n",
       "       -1.72393986e+00, -5.45824770e+00, -9.67273723e+00, -6.67900464e+01,\n",
       "       -4.34294154e+01, -2.89000924e+01, -3.40894184e+00, -1.07534901e+01,\n",
       "       -5.16691118e+00, -1.48674828e+01, -5.16691118e+00, -4.71985461e+00,\n",
       "       -1.14045570e+01, -2.65723179e+00, -5.37714836e-01, -2.89872607e+00,\n",
       "       -9.58490755e-03, -1.69263872e+01, -4.43538684e+00, -3.92603313e-01,\n",
       "       -5.56174397e-01, -2.71568614e+00, -3.54826220e+01, -2.89000924e+01,\n",
       "       -1.85793766e+02, -1.85793766e+02, -8.56678574e+00, -4.24141077e+00,\n",
       "       -6.79901984e-02, -6.70768171e+00, -3.12200353e+01, -5.44791120e+01,\n",
       "       -5.76492515e+01, -4.84338370e+01, -2.48707630e+00, -6.65463189e+00,\n",
       "       -4.35279867e+01, -8.21133366e-01, -1.48042183e+01, -1.73234837e+01,\n",
       "       -2.15428333e+01, -6.16014660e-01, -1.40301443e+01, -3.92603313e-01,\n",
       "       -4.41228965e+01, -1.29490738e-02, -4.55556484e+01, -3.78643546e+00,\n",
       "       -9.33148243e+00, -6.11852866e+00, -1.04014510e+01, -2.07408852e+00,\n",
       "       -2.70256448e+00, -4.65694071e+00, -1.72487801e-01, -2.16539700e+00,\n",
       "       -2.74918795e-02, -1.15058589e-01, -6.79901984e-02, -3.03694219e+01,\n",
       "       -2.11072545e+00, -4.65249596e+00, -1.86499577e+00, -9.99513766e+00,\n",
       "       -1.04928647e+01, -7.55152999e+00, -3.60116535e+01, -1.29490738e-02,\n",
       "       -2.07408852e+00, -1.73234837e+01, -9.33213791e+01, -1.66755705e+01,\n",
       "       -5.56635197e+01, -5.99987945e-01, -9.78228164e+00, -9.58490755e-03,\n",
       "       -4.32695051e+00, -1.47882454e+01, -1.07534901e+01, -4.99405256e+01,\n",
       "       -1.51995441e+01, -2.03208885e+01, -2.53881491e+01, -1.65673457e+01,\n",
       "       -4.14699861e+01, -1.07534901e+01, -5.33042511e+00, -7.29756797e+00,\n",
       "       -2.75846571e+01, -5.17186811e-01, -2.14795069e+01, -2.06180738e-01,\n",
       "       -3.12200353e+01, -2.42350354e+00, -1.96456368e+01, -2.58692664e+01,\n",
       "       -8.23394721e+00, -2.66745510e+01, -3.57935557e+01, -2.05425417e+01,\n",
       "       -5.86547892e+00, -3.18124590e+01, -1.75981847e-01, -1.90777485e+00,\n",
       "       -2.40313535e+00, -4.43538684e+00, -1.51190554e-01, -2.61303673e-01,\n",
       "       -4.47244402e+01, -4.13364701e-01, -8.33410057e+00, -6.92684164e+00,\n",
       "       -1.15721954e+00, -1.15721954e+00, -1.17243677e+02, -4.37670739e+01,\n",
       "       -1.16629732e+00, -1.11553234e+02, -3.21334196e+01, -7.18196254e+00,\n",
       "       -2.46435792e+00, -4.90518618e+00, -1.74123514e+01, -1.98261758e+01,\n",
       "       -4.01877223e+01, -8.99439615e-01, -1.34018435e+01, -4.03012186e+01,\n",
       "       -7.66863328e+00, -3.29183245e+01, -1.58667760e+01, -8.42803414e+00,\n",
       "       -4.67977341e+00, -1.47657800e+01, -1.10336292e-02, -1.15058589e-01,\n",
       "       -6.42346961e-01, -6.58606332e+00, -6.66627625e+00, -7.91798088e+00,\n",
       "       -1.48304437e+01, -2.64318792e-02, -7.44090531e+00, -5.28305235e-01,\n",
       "       -1.14684171e+01, -1.03414523e+01, -3.39377102e-02, -7.58123595e+00,\n",
       "       -7.74842230e+00, -8.94190790e-02, -8.09384884e-02, -4.54365642e+01,\n",
       "       -1.41140526e+01, -3.07579754e+01, -3.91284380e+00, -5.88043263e+00,\n",
       "       -3.92436810e+00, -2.75808267e+00, -1.83702289e-01, -2.07387025e+00,\n",
       "       -4.12916020e+00, -1.01722577e+00, -5.08353300e+00, -1.01467782e+01,\n",
       "       -1.97789373e+01, -7.09285156e+01, -8.47276098e-03, -5.71702729e+00,\n",
       "       -7.86198816e+00, -2.35859722e+00, -3.36788510e+00, -2.84924847e+01,\n",
       "       -3.18008270e+01, -4.55923166e+00, -6.88003729e+01, -7.17149316e+00,\n",
       "       -4.94993583e+01, -2.53016852e+01, -6.07286850e+01, -1.85344168e+02,\n",
       "       -5.89921650e+01, -1.04207991e+01, -5.72092847e+01, -5.18379999e+00,\n",
       "       -1.26415094e-01, -2.05638892e+00, -4.42118654e+01, -4.49064563e+01,\n",
       "       -1.66631623e+01, -1.03395519e+01, -1.34992151e+02, -1.69676219e+01,\n",
       "       -2.89448955e+02, -2.06974399e+01, -1.25051574e+02, -1.44905192e+02,\n",
       "       -1.23884653e+02, -5.00546480e+01, -4.13364701e-01, -2.34577604e+02,\n",
       "       -1.99088405e+01, -1.23537151e-01, -1.86386212e+02, -2.04808540e-01,\n",
       "       -8.06690157e+01, -1.47899420e+01, -2.75430770e-01, -5.95305638e-03,\n",
       "       -1.49726751e-01, -8.63932788e-01, -1.11156438e+01, -7.00853046e+01,\n",
       "       -8.49303469e+01, -2.17401579e+01, -8.72867547e+00, -5.42082969e+01,\n",
       "       -5.76616427e+01, -2.42205203e+01, -2.24153948e+01, -2.25523558e+01,\n",
       "       -5.07271492e-02, -2.26998788e+01, -3.16437445e+01, -1.86623193e+01,\n",
       "       -7.69355372e+01, -1.12801915e+01, -6.31334897e-01, -7.68002445e+00,\n",
       "       -1.43235862e+01, -1.03365290e+01, -3.00647280e-02, -1.05372851e+01,\n",
       "       -3.43774311e+01, -8.00620412e+01, -3.84314632e+00, -9.16549732e-01,\n",
       "       -6.36574958e+01, -2.02081766e+01, -5.43229349e+00, -1.62275765e+00,\n",
       "       -2.01113213e+00, -6.04809802e+01, -6.13959910e+01, -3.26284998e+00,\n",
       "       -6.48476825e+01, -5.10675623e+01, -9.96926676e+01, -6.29583618e+01,\n",
       "       -2.63878500e+01, -7.53932748e+01, -7.03805538e+00, -7.53932748e+01,\n",
       "       -5.92141501e+00, -1.32661105e+02, -3.45169047e-01, -6.65300161e-02,\n",
       "       -5.23682368e+01, -8.74921443e+01, -1.62275765e+00, -4.10622168e-01,\n",
       "       -1.52827762e+02, -2.85218299e+01, -2.87124380e-01, -1.61698064e+01])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.02001002e+00, 1.25092412e+00, 3.06805164e+00, 6.79901984e-02,\n",
       "       7.08255629e-01, 4.13566745e+01, 8.13755358e+01, 6.71494767e+01,\n",
       "       9.70498847e+01, 2.63430368e+01, 3.67428697e+00, 4.70741691e-01,\n",
       "       1.60507789e+00, 9.70498847e+01, 8.89557151e-01, 8.69418110e+00,\n",
       "       4.41228965e+01, 3.06562225e+01, 9.16549732e-01, 4.53185378e+01,\n",
       "       1.45705281e+00, 3.00983554e+00, 3.54617605e-03, 1.52964088e+01,\n",
       "       2.25022208e+01, 1.67905446e+01, 2.76735351e+00, 1.85354648e+01,\n",
       "       2.29957474e-01, 9.16549732e-01, 5.18379999e+00, 3.54617605e-03,\n",
       "       2.66745510e+01, 5.44791120e+01, 5.14078324e+01, 4.99405256e+01,\n",
       "       3.80360006e+01, 1.19884585e-02, 2.91033003e+00, 3.23104363e+00,\n",
       "       5.16691118e+00, 2.32487332e-01, 1.06678908e-02, 4.82615264e-01,\n",
       "       2.10211114e+01, 4.35585197e+01, 2.66745510e+01, 6.51231162e+01,\n",
       "       1.13690418e+01, 5.18379999e+00, 1.25085727e+00, 4.27873211e+00,\n",
       "       1.77161819e+00, 3.58044876e+01, 1.21519855e+01, 8.41044041e+00,\n",
       "       8.89557151e-01, 5.36657259e+00, 7.17595842e+01, 3.30230842e+01,\n",
       "       2.89239650e+01, 8.09034686e-01, 2.91033003e+00, 1.60507789e+00,\n",
       "       3.23104363e+00, 5.55648116e-01, 1.58374562e+01, 6.18633936e+00,\n",
       "       7.30069293e+00, 9.62066868e+00, 3.18079355e+01, 1.60507789e+00,\n",
       "       4.14699861e+01, 2.36253376e+01, 5.16691118e+00, 1.82205869e+01,\n",
       "       3.55599555e+01, 2.72424889e+01, 9.35761228e+00, 1.91401940e+01,\n",
       "       6.72450828e+00, 2.65440672e+00, 4.81922115e-01, 9.16549732e-01,\n",
       "       4.82615264e-01, 5.16691118e+00, 1.65673457e+01, 1.87496691e+01,\n",
       "       1.60507789e+00, 1.14041800e+01, 1.07534901e+01, 4.04602925e+00,\n",
       "       5.16691118e+00, 5.14127073e+01, 6.07551884e+01, 4.82615264e-01,\n",
       "       2.89000924e+01, 6.67900464e+01, 3.80360006e+01, 6.51231162e+01,\n",
       "       3.78643546e+00, 4.53185378e+01, 2.80008847e+01, 2.52995804e+00,\n",
       "       1.06678908e-02, 2.32487332e-01, 3.80360006e+01, 3.67700670e+01,\n",
       "       5.78250435e+01, 9.65326195e+00, 6.00999134e+01, 5.69004321e+01,\n",
       "       4.20960109e+00, 7.34161330e-02, 4.24141077e+00, 1.62504118e+02,\n",
       "       1.04124952e+01, 1.69263872e+01, 3.12256930e+01, 1.06661158e+01,\n",
       "       1.60507789e+00, 2.04714617e+00, 4.63627225e-01, 2.81308530e-01,\n",
       "       2.45422744e+01, 2.66745510e+01, 8.41826427e+01, 2.71568614e+00,\n",
       "       1.72393986e+00, 5.45824770e+00, 9.67273723e+00, 6.67900464e+01,\n",
       "       4.34294154e+01, 2.89000924e+01, 3.40894184e+00, 1.07534901e+01,\n",
       "       5.16691118e+00, 1.48674828e+01, 5.16691118e+00, 4.71985461e+00,\n",
       "       1.14045570e+01, 2.65723179e+00, 5.37714836e-01, 2.89872607e+00,\n",
       "       9.58490755e-03, 1.69263872e+01, 4.43538684e+00, 3.92603313e-01,\n",
       "       5.56174397e-01, 2.71568614e+00, 3.54826220e+01, 2.89000924e+01,\n",
       "       1.85793766e+02, 1.85793766e+02, 8.56678574e+00, 4.24141077e+00,\n",
       "       6.79901984e-02, 6.70768171e+00, 3.12200353e+01, 5.44791120e+01,\n",
       "       5.76492515e+01, 4.84338370e+01, 2.48707630e+00, 6.65463189e+00,\n",
       "       4.35279867e+01, 8.21133366e-01, 1.48042183e+01, 1.73234837e+01,\n",
       "       2.15428333e+01, 6.16014660e-01, 1.40301443e+01, 3.92603313e-01,\n",
       "       4.41228965e+01, 1.29490738e-02, 4.55556484e+01, 3.78643546e+00,\n",
       "       9.33148243e+00, 6.11852866e+00, 1.04014510e+01, 2.07408852e+00,\n",
       "       2.70256448e+00, 4.65694071e+00, 1.72487801e-01, 2.16539700e+00,\n",
       "       2.74918795e-02, 1.15058589e-01, 6.79901984e-02, 3.03694219e+01,\n",
       "       2.11072545e+00, 4.65249596e+00, 1.86499577e+00, 9.99513766e+00,\n",
       "       1.04928647e+01, 7.55152999e+00, 3.60116535e+01, 1.29490738e-02,\n",
       "       2.07408852e+00, 1.73234837e+01, 9.33213791e+01, 1.66755705e+01,\n",
       "       5.56635197e+01, 5.99987945e-01, 9.78228164e+00, 9.58490755e-03,\n",
       "       4.32695051e+00, 1.47882454e+01, 1.07534901e+01, 4.99405256e+01,\n",
       "       1.51995441e+01, 2.03208885e+01, 2.53881491e+01, 1.65673457e+01,\n",
       "       4.14699861e+01, 1.07534901e+01, 5.33042511e+00, 7.29756797e+00,\n",
       "       2.75846571e+01, 5.17186811e-01, 2.14795069e+01, 2.06180738e-01,\n",
       "       3.12200353e+01, 2.42350354e+00, 1.96456368e+01, 2.58692664e+01,\n",
       "       8.23394721e+00, 2.66745510e+01, 3.57935557e+01, 2.05425417e+01,\n",
       "       5.86547892e+00, 3.18124590e+01, 1.75981847e-01, 1.90777485e+00,\n",
       "       2.40313535e+00, 4.43538684e+00, 1.51190554e-01, 2.61303673e-01,\n",
       "       4.47244402e+01, 4.13364701e-01, 8.33410057e+00, 6.92684164e+00,\n",
       "       1.15721954e+00, 1.15721954e+00, 1.17243677e+02, 4.37670739e+01,\n",
       "       1.16629732e+00, 1.11553234e+02, 3.21334196e+01, 7.18196254e+00,\n",
       "       2.46435792e+00, 4.90518618e+00, 1.74123514e+01, 1.98261758e+01,\n",
       "       4.01877223e+01, 8.99439615e-01, 1.34018435e+01, 4.03012186e+01,\n",
       "       7.66863328e+00, 3.29183245e+01, 1.58667760e+01, 8.42803414e+00,\n",
       "       4.67977341e+00, 1.47657800e+01, 1.10336292e-02, 1.15058589e-01,\n",
       "       6.42346961e-01, 6.58606332e+00, 6.66627625e+00, 7.91798088e+00,\n",
       "       1.48304437e+01, 2.64318792e-02, 7.44090531e+00, 5.28305235e-01,\n",
       "       1.14684171e+01, 1.03414523e+01, 3.39377102e-02, 7.58123595e+00,\n",
       "       7.74842230e+00, 8.94190790e-02, 8.09384884e-02, 4.54365642e+01,\n",
       "       1.41140526e+01, 3.07579754e+01, 3.91284380e+00, 5.88043263e+00,\n",
       "       3.92436810e+00, 2.75808267e+00, 1.83702289e-01, 2.07387025e+00,\n",
       "       4.12916020e+00, 1.01722577e+00, 5.08353300e+00, 1.01467782e+01,\n",
       "       1.97789373e+01, 7.09285156e+01, 8.47276098e-03, 5.71702729e+00,\n",
       "       7.86198816e+00, 2.35859722e+00, 3.36788510e+00, 2.84924847e+01,\n",
       "       3.18008270e+01, 4.55923166e+00, 6.88003729e+01, 7.17149316e+00,\n",
       "       4.94993583e+01, 2.53016852e+01, 6.07286850e+01, 1.85344168e+02,\n",
       "       5.89921650e+01, 1.04207991e+01, 5.72092847e+01, 5.18379999e+00,\n",
       "       1.26415094e-01, 2.05638892e+00, 4.42118654e+01, 4.49064563e+01,\n",
       "       1.66631623e+01, 1.03395519e+01, 1.34992151e+02, 1.69676219e+01,\n",
       "       2.89448955e+02, 2.06974399e+01, 1.25051574e+02, 1.44905192e+02,\n",
       "       1.23884653e+02, 5.00546480e+01, 4.13364701e-01, 2.34577604e+02,\n",
       "       1.99088405e+01, 1.23537151e-01, 1.86386212e+02, 2.04808540e-01,\n",
       "       8.06690157e+01, 1.47899420e+01, 2.75430770e-01, 5.95305638e-03,\n",
       "       1.49726751e-01, 8.63932788e-01, 1.11156438e+01, 7.00853046e+01,\n",
       "       8.49303469e+01, 2.17401579e+01, 8.72867547e+00, 5.42082969e+01,\n",
       "       5.76616427e+01, 2.42205203e+01, 2.24153948e+01, 2.25523558e+01,\n",
       "       5.07271492e-02, 2.26998788e+01, 3.16437445e+01, 1.86623193e+01,\n",
       "       7.69355372e+01, 1.12801915e+01, 6.31334897e-01, 7.68002445e+00,\n",
       "       1.43235862e+01, 1.03365290e+01, 3.00647280e-02, 1.05372851e+01,\n",
       "       3.43774311e+01, 8.00620412e+01, 3.84314632e+00, 9.16549732e-01,\n",
       "       6.36574958e+01, 2.02081766e+01, 5.43229349e+00, 1.62275765e+00,\n",
       "       2.01113213e+00, 6.04809802e+01, 6.13959910e+01, 3.26284998e+00,\n",
       "       6.48476825e+01, 5.10675623e+01, 9.96926676e+01, 6.29583618e+01,\n",
       "       2.63878500e+01, 7.53932748e+01, 7.03805538e+00, 7.53932748e+01,\n",
       "       5.92141501e+00, 1.32661105e+02, 3.45169047e-01, 6.65300161e-02,\n",
       "       5.23682368e+01, 8.74921443e+01, 1.62275765e+00, 4.10622168e-01,\n",
       "       1.52827762e+02, 2.85218299e+01, 2.87124380e-01, 1.61698064e+01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For higher order polynomial fit, we use pipline tool (http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). Below shows how to fit an order 1 to 5 polynomial data and show the loocv results\n",
    "PolynomialFeatures: Generate polynomial and interaction features. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.231513517929226, 19.24821312448941, 19.33498406411397, 19.424430309374937, 19.03322024895203]\n"
     ]
    }
   ],
   "source": [
    "A = []\n",
    "for porder in range(1, 6):\n",
    "    model = Pipeline([('poly', PolynomialFeatures(degree=porder)), ('linear', LinearRegression())])\n",
    "    k_fold = KFold(n_splits=x.shape[0]) # loocv use folds equal to # of observations\n",
    "    test = cross_val_score(model, x, y, cv=k_fold,  scoring = 'neg_mean_squared_error', n_jobs=-1)\n",
    "    A.append(np.mean(-test))\n",
    "    \n",
    "print (A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.3 k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold validation is exactly same as LOOCV with different n_splits parameter setup. The computation time is much shorter than that of LOOCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27.439933652339857, 21.235840055802118, 21.336606183328527, 21.35388699303018, 20.90563774089812, 20.80193369142537, 20.95328706780984, 21.07715570228614, 21.03690306486462, 20.980936037598624]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "A = []\n",
    "for porder in range(1, 11):\n",
    "    model = Pipeline([('poly', PolynomialFeatures(degree=porder)), ('linear', LinearRegression())])\n",
    "    k_fold = KFold(n_splits=10) \n",
    "    test = cross_val_score(model, x, y, cv = k_fold,  scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "    A.append(np.mean(-test))\n",
    "    \n",
    "print (A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.4 The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap means sampling with replacement. To eliminate the effect of sample size, the norm practice is to sample the same size as original dataset with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Portfolio = pd.read_csv('data/Portfolio.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Portfolio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.895251</td>\n",
       "      <td>-0.234924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.562454</td>\n",
       "      <td>-0.885176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.417090</td>\n",
       "      <td>0.271888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.044356</td>\n",
       "      <td>-0.734198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.315568</td>\n",
       "      <td>0.841983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.737124</td>\n",
       "      <td>-2.037191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.966413</td>\n",
       "      <td>1.452957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.152868</td>\n",
       "      <td>-0.434139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.081208</td>\n",
       "      <td>1.450809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.891782</td>\n",
       "      <td>0.821016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.293202</td>\n",
       "      <td>-1.042391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.505779</td>\n",
       "      <td>0.608478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.526751</td>\n",
       "      <td>-0.222493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.066469</td>\n",
       "      <td>1.231357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.294016</td>\n",
       "      <td>0.628589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.042549</td>\n",
       "      <td>-1.267574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.830970</td>\n",
       "      <td>-0.572752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.326937</td>\n",
       "      <td>-0.487472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.521480</td>\n",
       "      <td>2.565985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.399868</td>\n",
       "      <td>-0.357836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.645448</td>\n",
       "      <td>-1.412431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.904352</td>\n",
       "      <td>-0.568305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.764586</td>\n",
       "      <td>-0.746273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.810485</td>\n",
       "      <td>0.493747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.169899</td>\n",
       "      <td>-2.725281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.685376</td>\n",
       "      <td>-0.457616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.090918</td>\n",
       "      <td>0.014495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.432340</td>\n",
       "      <td>-0.399831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.268815</td>\n",
       "      <td>-0.201608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.851841</td>\n",
       "      <td>-1.741829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-0.984357</td>\n",
       "      <td>-1.139160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-1.384992</td>\n",
       "      <td>0.702700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-0.358843</td>\n",
       "      <td>-1.694513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-0.226618</td>\n",
       "      <td>0.801939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-0.941077</td>\n",
       "      <td>-0.733189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2.460336</td>\n",
       "      <td>-0.048373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.716797</td>\n",
       "      <td>0.602337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-0.248087</td>\n",
       "      <td>-1.018490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.010773</td>\n",
       "      <td>0.052978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2.313049</td>\n",
       "      <td>1.752359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.835180</td>\n",
       "      <td>0.985715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-1.071903</td>\n",
       "      <td>-1.247298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-1.650526</td>\n",
       "      <td>0.215465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>-0.600486</td>\n",
       "      <td>-0.420941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-0.058529</td>\n",
       "      <td>0.127621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.075727</td>\n",
       "      <td>-0.522149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-1.157832</td>\n",
       "      <td>0.590894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.673606</td>\n",
       "      <td>0.114623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>-1.043988</td>\n",
       "      <td>-0.418944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.014687</td>\n",
       "      <td>-0.558747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.675322</td>\n",
       "      <td>1.482630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.778342</td>\n",
       "      <td>0.942774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-1.295764</td>\n",
       "      <td>-1.085204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.079602</td>\n",
       "      <td>-0.539101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2.260858</td>\n",
       "      <td>0.673225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.479091</td>\n",
       "      <td>1.454774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.535020</td>\n",
       "      <td>-0.399175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.773129</td>\n",
       "      <td>-0.957175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.403634</td>\n",
       "      <td>1.396038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.588496</td>\n",
       "      <td>-0.497285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X         Y\n",
       "0  -0.895251 -0.234924\n",
       "1  -1.562454 -0.885176\n",
       "2  -0.417090  0.271888\n",
       "3   1.044356 -0.734198\n",
       "4  -0.315568  0.841983\n",
       "5  -1.737124 -2.037191\n",
       "6   1.966413  1.452957\n",
       "7   2.152868 -0.434139\n",
       "8  -0.081208  1.450809\n",
       "9  -0.891782  0.821016\n",
       "10 -0.293202 -1.042391\n",
       "11  0.505779  0.608478\n",
       "12  0.526751 -0.222493\n",
       "13  1.066469  1.231357\n",
       "14  0.294016  0.628589\n",
       "15  0.042549 -1.267574\n",
       "16  1.830970 -0.572752\n",
       "17 -0.326937 -0.487472\n",
       "18  0.521480  2.565985\n",
       "19  1.399868 -0.357836\n",
       "20 -0.645448 -1.412431\n",
       "21 -0.904352 -0.568305\n",
       "22 -1.764586 -0.746273\n",
       "23 -1.810485  0.493747\n",
       "24 -1.169899 -2.725281\n",
       "25 -0.685376 -0.457616\n",
       "26  1.090918  0.014495\n",
       "27 -0.432340 -0.399831\n",
       "28  0.268815 -0.201608\n",
       "29 -0.851841 -1.741829\n",
       "..       ...       ...\n",
       "70 -0.984357 -1.139160\n",
       "71 -1.384992  0.702700\n",
       "72 -0.358843 -1.694513\n",
       "73 -0.226618  0.801939\n",
       "74 -0.941077 -0.733189\n",
       "75  2.460336 -0.048373\n",
       "76  0.716797  0.602337\n",
       "77 -0.248087 -1.018490\n",
       "78  1.010773  0.052978\n",
       "79  2.313049  1.752359\n",
       "80  0.835180  0.985715\n",
       "81 -1.071903 -1.247298\n",
       "82 -1.650526  0.215465\n",
       "83 -0.600486 -0.420941\n",
       "84 -0.058529  0.127621\n",
       "85  0.075727 -0.522149\n",
       "86 -1.157832  0.590894\n",
       "87  1.673606  0.114623\n",
       "88 -1.043988 -0.418944\n",
       "89  0.014687 -0.558747\n",
       "90  0.675322  1.482630\n",
       "91  1.778342  0.942774\n",
       "92 -1.295764 -1.085204\n",
       "93  0.079602 -0.539101\n",
       "94  2.260858  0.673225\n",
       "95  0.479091  1.454774\n",
       "96 -0.535020 -0.399175\n",
       "97 -0.773129 -0.957175\n",
       "98  0.403634  1.396038\n",
       "99 -0.588496 -0.497285\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To illustrate the use of the bootstrap on this data, we must first create a function, alpha_fn(), which takes as input the (X, Y) data as well as a vector indicating which observations should be used to estimate alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_fn(data, index):\n",
    "    X = data.X[index]\n",
    "    Y = data.Y[index]\n",
    "    return (np.var(Y) - np.cov(X,Y)[0,1])/(np.var(X) + np.var(Y) - 2 * np.cov(X, Y)[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.arange(0, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5766511516104118"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_fn(Portfolio, np.arange(0, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.12864236, 0.62635829],\n",
       "       [0.62635829, 1.30823747]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(Portfolio.X,Portfolio.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6263582921063724"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(Portfolio.X,Portfolio.Y)[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate one set of random index with 100 elements. The array has been sorted to show there are repeat elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  1,  1,  2,  2,  3,  7,  9, 10, 13, 14, 16, 16, 18, 18, 19,\n",
       "       20, 20, 21, 21, 21, 21, 22, 24, 24, 26, 28, 28, 29, 29, 30, 32, 32,\n",
       "       33, 33, 33, 33, 36, 37, 37, 37, 37, 38, 39, 39, 41, 43, 44, 44, 46,\n",
       "       48, 48, 48, 48, 49, 51, 52, 54, 54, 55, 55, 56, 56, 56, 56, 59, 60,\n",
       "       60, 61, 62, 63, 63, 64, 66, 69, 71, 72, 74, 74, 74, 75, 78, 79, 80,\n",
       "       81, 85, 88, 90, 90, 90, 91, 91, 93, 94, 95, 96, 96, 97, 99])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.random.choice(range(0, 100), size=100, replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall the previous function with a random set of input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5699422338741068"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_fn(Portfolio, np.random.choice(range(0, 100), size=100, replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an ad hoc function called boot_python()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_python(data, input_fun, iteration):\n",
    "    n = Portfolio.shape[0]\n",
    "    idx = np.random.randint(0, n, (iteration, n)) # Return random integers from low (inclusive) to high (exclusive).\n",
    "                                                  # size: output shape, e.g., (1000, 100)\n",
    "    stat = np.zeros(iteration)\n",
    "    for i in range(len(idx)):\n",
    "        stat[i] = input_fun(data, idx[i])\n",
    "    \n",
    "    return {'Mean': np.mean(stat), 'STD': np.std(stat)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mean': 0.5783005990543031, 'STD': 0.0912134132867913}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boot_python(Portfolio, alpha_fn, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[62, 56, 83, ..., 49, 51,  0],\n",
       "       [16, 62,  3, ...,  5, 93, 52],\n",
       "       [95, 68, 13, ..., 69, 78, 50],\n",
       "       ...,\n",
       "       [ 7, 94, 59, ..., 80, 22, 11],\n",
       "       [40, 11, 39, ...,  1, 52, 60],\n",
       "       [96, 55, 79, ..., 75, 96, 95]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100\n",
    "iteration = 1000\n",
    "idx = np.random.randint(0, n, (iteration, n))\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar idea can be used in a lot of other places, such as estimating the accuracy of a linear regression model coeffcients."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
