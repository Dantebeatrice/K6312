{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Lab: Logistic Regression, LDA, QDA, and KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.1 The Stock Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import math\n",
    "from patsy import dmatrices\n",
    "\n",
    "\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by examining some numerical and graphical summaries of\n",
    "the Smarket data. This data set consists of\n",
    "percentage returns for the S&P 500 stock index over 1, 250 days, from the\n",
    "beginning of 2001 until the end of 2005. For each date, we have recorded\n",
    "the percentage returns for each of the five previous trading days, Lag1\n",
    "through Lag5. We have also recorded Volume (the number of shares traded \n",
    "on the previous day, in billions), Today (the percentage return on the date\n",
    "in question) and Direction (whether the market was Up or Down on this\n",
    "date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "Smarket = pd.read_csv('data/Smarket.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>5.010</td>\n",
       "      <td>1.1913</td>\n",
       "      <td>0.959</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.2965</td>\n",
       "      <td>1.032</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>1.4112</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.2760</td>\n",
       "      <td>0.614</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>0.213</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year   Lag1   Lag2   Lag3   Lag4   Lag5  Volume  Today Direction\n",
       "0  2001  0.381 -0.192 -2.624 -1.055  5.010  1.1913  0.959        Up\n",
       "1  2001  0.959  0.381 -0.192 -2.624 -1.055  1.2965  1.032        Up\n",
       "2  2001  1.032  0.959  0.381 -0.192 -2.624  1.4112 -0.623      Down\n",
       "3  2001 -0.623  1.032  0.959  0.381 -0.192  1.2760  0.614        Up\n",
       "4  2001  0.614 -0.623  1.032  0.959  0.381  1.2057  0.213        Up"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>2005</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>1.88850</td>\n",
       "      <td>0.043</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>2005</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>1.28581</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>2005</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>1.54047</td>\n",
       "      <td>0.130</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>2005</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.252</td>\n",
       "      <td>1.42236</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>2005</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.422</td>\n",
       "      <td>1.38254</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year   Lag1   Lag2   Lag3   Lag4   Lag5   Volume  Today Direction\n",
       "1245  2005  0.422  0.252 -0.024 -0.584 -0.285  1.88850  0.043        Up\n",
       "1246  2005  0.043  0.422  0.252 -0.024 -0.584  1.28581 -0.955      Down\n",
       "1247  2005 -0.955  0.043  0.422  0.252 -0.024  1.54047  0.130        Up\n",
       "1248  2005  0.130 -0.955  0.043  0.422  0.252  1.42236 -0.298      Down\n",
       "1249  2005 -0.298  0.130 -0.955  0.043  0.422  1.38254 -0.489      Down"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1250.000000</td>\n",
       "      <td>1250.000000</td>\n",
       "      <td>1250.000000</td>\n",
       "      <td>1250.000000</td>\n",
       "      <td>1250.000000</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>1250.000000</td>\n",
       "      <td>1250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2003.016000</td>\n",
       "      <td>0.003834</td>\n",
       "      <td>0.003919</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.00561</td>\n",
       "      <td>1.478305</td>\n",
       "      <td>0.003138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.409018</td>\n",
       "      <td>1.136299</td>\n",
       "      <td>1.136280</td>\n",
       "      <td>1.138703</td>\n",
       "      <td>1.138774</td>\n",
       "      <td>1.14755</td>\n",
       "      <td>0.360357</td>\n",
       "      <td>1.136334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2001.000000</td>\n",
       "      <td>-4.922000</td>\n",
       "      <td>-4.922000</td>\n",
       "      <td>-4.922000</td>\n",
       "      <td>-4.922000</td>\n",
       "      <td>-4.92200</td>\n",
       "      <td>0.356070</td>\n",
       "      <td>-4.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2002.000000</td>\n",
       "      <td>-0.639500</td>\n",
       "      <td>-0.639500</td>\n",
       "      <td>-0.640000</td>\n",
       "      <td>-0.640000</td>\n",
       "      <td>-0.64000</td>\n",
       "      <td>1.257400</td>\n",
       "      <td>-0.639500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2003.000000</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>0.03850</td>\n",
       "      <td>1.422950</td>\n",
       "      <td>0.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2004.000000</td>\n",
       "      <td>0.596750</td>\n",
       "      <td>0.596750</td>\n",
       "      <td>0.596750</td>\n",
       "      <td>0.596750</td>\n",
       "      <td>0.59700</td>\n",
       "      <td>1.641675</td>\n",
       "      <td>0.596750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2005.000000</td>\n",
       "      <td>5.733000</td>\n",
       "      <td>5.733000</td>\n",
       "      <td>5.733000</td>\n",
       "      <td>5.733000</td>\n",
       "      <td>5.73300</td>\n",
       "      <td>3.152470</td>\n",
       "      <td>5.733000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Year         Lag1         Lag2         Lag3         Lag4  \\\n",
       "count  1250.000000  1250.000000  1250.000000  1250.000000  1250.000000   \n",
       "mean   2003.016000     0.003834     0.003919     0.001716     0.001636   \n",
       "std       1.409018     1.136299     1.136280     1.138703     1.138774   \n",
       "min    2001.000000    -4.922000    -4.922000    -4.922000    -4.922000   \n",
       "25%    2002.000000    -0.639500    -0.639500    -0.640000    -0.640000   \n",
       "50%    2003.000000     0.039000     0.039000     0.038500     0.038500   \n",
       "75%    2004.000000     0.596750     0.596750     0.596750     0.596750   \n",
       "max    2005.000000     5.733000     5.733000     5.733000     5.733000   \n",
       "\n",
       "             Lag5       Volume        Today  \n",
       "count  1250.00000  1250.000000  1250.000000  \n",
       "mean      0.00561     1.478305     0.003138  \n",
       "std       1.14755     0.360357     1.136334  \n",
       "min      -4.92200     0.356070    -4.922000  \n",
       "25%      -0.64000     1.257400    -0.639500  \n",
       "50%       0.03850     1.422950     0.038500  \n",
       "75%       0.59700     1.641675     0.596750  \n",
       "max       5.73300     3.152470     5.733000  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 9)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For panda data frame, there is a method corr to compute pairwise correlation between numerical variables\n",
    "As one would expect, the correlations between the lag variables and todayâ€™s returns are close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.030596</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.035689</td>\n",
       "      <td>0.029788</td>\n",
       "      <td>0.539006</td>\n",
       "      <td>0.030095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag1</th>\n",
       "      <td>0.029700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>-0.026155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag2</th>\n",
       "      <td>0.030596</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.043383</td>\n",
       "      <td>-0.010250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag3</th>\n",
       "      <td>0.033195</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024051</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.002448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag4</th>\n",
       "      <td>0.035689</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.024051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.027084</td>\n",
       "      <td>-0.048414</td>\n",
       "      <td>-0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag5</th>\n",
       "      <td>0.029788</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.027084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>-0.034860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>0.539006</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>-0.043383</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.048414</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Today</th>\n",
       "      <td>0.030095</td>\n",
       "      <td>-0.026155</td>\n",
       "      <td>-0.010250</td>\n",
       "      <td>-0.002448</td>\n",
       "      <td>-0.006900</td>\n",
       "      <td>-0.034860</td>\n",
       "      <td>0.014592</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Year      Lag1      Lag2      Lag3      Lag4      Lag5    Volume  \\\n",
       "Year    1.000000  0.029700  0.030596  0.033195  0.035689  0.029788  0.539006   \n",
       "Lag1    0.029700  1.000000 -0.026294 -0.010803 -0.002986 -0.005675  0.040910   \n",
       "Lag2    0.030596 -0.026294  1.000000 -0.025897 -0.010854 -0.003558 -0.043383   \n",
       "Lag3    0.033195 -0.010803 -0.025897  1.000000 -0.024051 -0.018808 -0.041824   \n",
       "Lag4    0.035689 -0.002986 -0.010854 -0.024051  1.000000 -0.027084 -0.048414   \n",
       "Lag5    0.029788 -0.005675 -0.003558 -0.018808 -0.027084  1.000000 -0.022002   \n",
       "Volume  0.539006  0.040910 -0.043383 -0.041824 -0.048414 -0.022002  1.000000   \n",
       "Today   0.030095 -0.026155 -0.010250 -0.002448 -0.006900 -0.034860  0.014592   \n",
       "\n",
       "           Today  \n",
       "Year    0.030095  \n",
       "Lag1   -0.026155  \n",
       "Lag2   -0.010250  \n",
       "Lag3   -0.002448  \n",
       "Lag4   -0.006900  \n",
       "Lag5   -0.034860  \n",
       "Volume  0.014592  \n",
       "Today   1.000000  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>1.985332</td>\n",
       "      <td>0.047551</td>\n",
       "      <td>0.048986</td>\n",
       "      <td>0.053259</td>\n",
       "      <td>0.057264</td>\n",
       "      <td>0.048165</td>\n",
       "      <td>0.273680</td>\n",
       "      <td>0.048186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag1</th>\n",
       "      <td>0.047551</td>\n",
       "      <td>1.291175</td>\n",
       "      <td>-0.033950</td>\n",
       "      <td>-0.013979</td>\n",
       "      <td>-0.003864</td>\n",
       "      <td>-0.007399</td>\n",
       "      <td>0.016752</td>\n",
       "      <td>-0.033772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag2</th>\n",
       "      <td>0.048986</td>\n",
       "      <td>-0.033950</td>\n",
       "      <td>1.291133</td>\n",
       "      <td>-0.033507</td>\n",
       "      <td>-0.014044</td>\n",
       "      <td>-0.004639</td>\n",
       "      <td>-0.017764</td>\n",
       "      <td>-0.013235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag3</th>\n",
       "      <td>0.053259</td>\n",
       "      <td>-0.013979</td>\n",
       "      <td>-0.033507</td>\n",
       "      <td>1.296644</td>\n",
       "      <td>-0.031188</td>\n",
       "      <td>-0.024577</td>\n",
       "      <td>-0.017162</td>\n",
       "      <td>-0.003167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag4</th>\n",
       "      <td>0.057264</td>\n",
       "      <td>-0.003864</td>\n",
       "      <td>-0.014044</td>\n",
       "      <td>-0.031188</td>\n",
       "      <td>1.296806</td>\n",
       "      <td>-0.035393</td>\n",
       "      <td>-0.019868</td>\n",
       "      <td>-0.008928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag5</th>\n",
       "      <td>0.048165</td>\n",
       "      <td>-0.007399</td>\n",
       "      <td>-0.004639</td>\n",
       "      <td>-0.024577</td>\n",
       "      <td>-0.035393</td>\n",
       "      <td>1.316871</td>\n",
       "      <td>-0.009099</td>\n",
       "      <td>-0.045458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>0.273680</td>\n",
       "      <td>0.016752</td>\n",
       "      <td>-0.017764</td>\n",
       "      <td>-0.017162</td>\n",
       "      <td>-0.019868</td>\n",
       "      <td>-0.009099</td>\n",
       "      <td>0.129857</td>\n",
       "      <td>0.005975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Today</th>\n",
       "      <td>0.048186</td>\n",
       "      <td>-0.033772</td>\n",
       "      <td>-0.013235</td>\n",
       "      <td>-0.003167</td>\n",
       "      <td>-0.008928</td>\n",
       "      <td>-0.045458</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>1.291255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Year      Lag1      Lag2      Lag3      Lag4      Lag5    Volume  \\\n",
       "Year    1.985332  0.047551  0.048986  0.053259  0.057264  0.048165  0.273680   \n",
       "Lag1    0.047551  1.291175 -0.033950 -0.013979 -0.003864 -0.007399  0.016752   \n",
       "Lag2    0.048986 -0.033950  1.291133 -0.033507 -0.014044 -0.004639 -0.017764   \n",
       "Lag3    0.053259 -0.013979 -0.033507  1.296644 -0.031188 -0.024577 -0.017162   \n",
       "Lag4    0.057264 -0.003864 -0.014044 -0.031188  1.296806 -0.035393 -0.019868   \n",
       "Lag5    0.048165 -0.007399 -0.004639 -0.024577 -0.035393  1.316871 -0.009099   \n",
       "Volume  0.273680  0.016752 -0.017764 -0.017162 -0.019868 -0.009099  0.129857   \n",
       "Today   0.048186 -0.033772 -0.013235 -0.003167 -0.008928 -0.045458  0.005975   \n",
       "\n",
       "           Today  \n",
       "Year    0.048186  \n",
       "Lag1   -0.033772  \n",
       "Lag2   -0.013235  \n",
       "Lag3   -0.003167  \n",
       "Lag4   -0.008928  \n",
       "Lag5   -0.045458  \n",
       "Volume  0.005975  \n",
       "Today   1.291255  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.cov() # covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Smarket.iloc[:, 6])\n",
    "# plt.plot(Smarket.ix[:, 6]) # behave like loc but falls back to behaving like iloc if the label is not in the index\n",
    "# plt.plot(Smarket[['Volume']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Statsmodels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "patsy is a Python package for describing statistical models (especially linear models, or models that have a linear component) and building design matrices. It is closely inspired by and compatible with the formula mini-language used in R and S.\n",
    "Generate design matrices suitable for regressing y onto X. The return value is a Python tuple containing two DesignMatrix objects, \n",
    "the first representing the left-hand side of our formula, and the second representing the right-hand side. Notice that an intercept term was automatically added to the right-hand side.\n",
    "\n",
    "Help for dmatrices: http://patsy.readthedocs.io/en/latest/quickstart.html\n",
    "\n",
    "Overview for dmatrices: http://patsy.readthedocs.io/en/latest/overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Direction[Down]  Direction[Up]\n",
      "0                 0.0            1.0\n",
      "1                 0.0            1.0\n",
      "2                 1.0            0.0\n",
      "3                 0.0            1.0\n",
      "4                 0.0            1.0\n",
      "5                 0.0            1.0\n",
      "6                 1.0            0.0\n",
      "7                 0.0            1.0\n",
      "8                 0.0            1.0\n",
      "9                 0.0            1.0\n",
      "10                1.0            0.0\n",
      "11                1.0            0.0\n",
      "12                0.0            1.0\n",
      "13                0.0            1.0\n",
      "14                1.0            0.0\n",
      "15                0.0            1.0\n",
      "16                1.0            0.0\n",
      "17                0.0            1.0\n",
      "18                1.0            0.0\n",
      "19                1.0            0.0\n",
      "20                1.0            0.0\n",
      "21                1.0            0.0\n",
      "22                0.0            1.0\n",
      "23                1.0            0.0\n",
      "24                1.0            0.0\n",
      "25                0.0            1.0\n",
      "26                1.0            0.0\n",
      "27                1.0            0.0\n",
      "28                1.0            0.0\n",
      "29                1.0            0.0\n",
      "...               ...            ...\n",
      "1220              0.0            1.0\n",
      "1221              0.0            1.0\n",
      "1222              0.0            1.0\n",
      "1223              0.0            1.0\n",
      "1224              0.0            1.0\n",
      "1225              0.0            1.0\n",
      "1226              1.0            0.0\n",
      "1227              0.0            1.0\n",
      "1228              1.0            0.0\n",
      "1229              0.0            1.0\n",
      "1230              0.0            1.0\n",
      "1231              1.0            0.0\n",
      "1232              0.0            1.0\n",
      "1233              1.0            0.0\n",
      "1234              1.0            0.0\n",
      "1235              0.0            1.0\n",
      "1236              0.0            1.0\n",
      "1237              0.0            1.0\n",
      "1238              0.0            1.0\n",
      "1239              1.0            0.0\n",
      "1240              1.0            0.0\n",
      "1241              1.0            0.0\n",
      "1242              1.0            0.0\n",
      "1243              0.0            1.0\n",
      "1244              0.0            1.0\n",
      "1245              0.0            1.0\n",
      "1246              1.0            0.0\n",
      "1247              0.0            1.0\n",
      "1248              1.0            0.0\n",
      "1249              1.0            0.0\n",
      "\n",
      "[1250 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "y, X = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket, return_type = 'dataframe')\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Intercept   Lag1   Lag2   Lag3   Lag4   Lag5   Volume\n",
      "0           1.0  0.381 -0.192 -2.624 -1.055  5.010  1.19130\n",
      "1           1.0  0.959  0.381 -0.192 -2.624 -1.055  1.29650\n",
      "2           1.0  1.032  0.959  0.381 -0.192 -2.624  1.41120\n",
      "3           1.0 -0.623  1.032  0.959  0.381 -0.192  1.27600\n",
      "4           1.0  0.614 -0.623  1.032  0.959  0.381  1.20570\n",
      "5           1.0  0.213  0.614 -0.623  1.032  0.959  1.34910\n",
      "6           1.0  1.392  0.213  0.614 -0.623  1.032  1.44500\n",
      "7           1.0 -0.403  1.392  0.213  0.614 -0.623  1.40780\n",
      "8           1.0  0.027 -0.403  1.392  0.213  0.614  1.16400\n",
      "9           1.0  1.303  0.027 -0.403  1.392  0.213  1.23260\n",
      "10          1.0  0.287  1.303  0.027 -0.403  1.392  1.30900\n",
      "11          1.0 -0.498  0.287  1.303  0.027 -0.403  1.25800\n",
      "12          1.0 -0.189 -0.498  0.287  1.303  0.027  1.09800\n",
      "13          1.0  0.680 -0.189 -0.498  0.287  1.303  1.05310\n",
      "14          1.0  0.701  0.680 -0.189 -0.498  0.287  1.14980\n",
      "15          1.0 -0.562  0.701  0.680 -0.189 -0.498  1.29530\n",
      "16          1.0  0.546 -0.562  0.701  0.680 -0.189  1.11880\n",
      "17          1.0 -1.747  0.546 -0.562  0.701  0.680  1.04840\n",
      "18          1.0  0.359 -1.747  0.546 -0.562  0.701  1.01300\n",
      "19          1.0 -0.151  0.359 -1.747  0.546 -0.562  1.05960\n",
      "20          1.0 -0.841 -0.151  0.359 -1.747  0.546  1.15830\n",
      "21          1.0 -0.623 -0.841 -0.151  0.359 -1.747  1.10720\n",
      "22          1.0 -1.334 -0.623 -0.841 -0.151  0.359  1.07550\n",
      "23          1.0  1.183 -1.334 -0.623 -0.841 -0.151  1.03910\n",
      "24          1.0 -0.865  1.183 -1.334 -0.623 -0.841  1.07520\n",
      "25          1.0 -0.218 -0.865  1.183 -1.334 -0.623  1.15030\n",
      "26          1.0  0.812 -0.218 -0.865  1.183 -1.334  1.15370\n",
      "27          1.0 -1.891  0.812 -0.218 -0.865  1.183  1.25720\n",
      "28          1.0 -1.736 -1.891  0.812 -0.218 -0.865  1.11220\n",
      "29          1.0 -1.851 -1.736 -1.891  0.812 -0.218  1.20850\n",
      "...         ...    ...    ...    ...    ...    ...      ...\n",
      "1220        1.0  0.179 -0.385 -0.078  0.305  0.845  2.12158\n",
      "1221        1.0  0.941  0.179 -0.385 -0.078  0.305  2.29804\n",
      "1222        1.0  0.440  0.941  0.179 -0.385 -0.078  2.45329\n",
      "1223        1.0  0.527  0.440  0.941  0.179 -0.385  2.11735\n",
      "1224        1.0  0.508  0.527  0.440  0.941  0.179  2.29142\n",
      "1225        1.0  0.347  0.508  0.527  0.440  0.941  1.98540\n",
      "1226        1.0  0.209  0.347  0.508  0.527  0.440  0.72494\n",
      "1227        1.0 -0.851  0.209  0.347  0.508  0.527  2.01690\n",
      "1228        1.0  0.002 -0.851  0.209  0.347  0.508  2.26834\n",
      "1229        1.0 -0.636  0.002 -0.851  0.209  0.347  2.37469\n",
      "1230        1.0  1.216 -0.636  0.002 -0.851  0.209  2.61483\n",
      "1231        1.0  0.032  1.216 -0.636  0.002 -0.851  2.12558\n",
      "1232        1.0 -0.236  0.032  1.216 -0.636  0.002  2.32584\n",
      "1233        1.0  0.128 -0.236  0.032  1.216 -0.636  2.11074\n",
      "1234        1.0 -0.501  0.128 -0.236  0.032  1.216  2.09383\n",
      "1235        1.0 -0.122 -0.501  0.128 -0.236  0.032  2.17830\n",
      "1236        1.0  0.281 -0.122 -0.501  0.128 -0.236  1.89629\n",
      "1237        1.0  0.084  0.281 -0.122 -0.501  0.128  1.87655\n",
      "1238        1.0  0.555  0.084  0.281 -0.122 -0.501  2.39002\n",
      "1239        1.0  0.419  0.555  0.084  0.281 -0.122  2.14552\n",
      "1240        1.0 -0.141  0.419  0.555  0.084  0.281  2.18059\n",
      "1241        1.0 -0.285 -0.141  0.419  0.555  0.084  2.58419\n",
      "1242        1.0 -0.584 -0.285 -0.141  0.419  0.555  2.20881\n",
      "1243        1.0 -0.024 -0.584 -0.285 -0.141  0.419  1.99669\n",
      "1244        1.0  0.252 -0.024 -0.584 -0.285 -0.141  2.06517\n",
      "1245        1.0  0.422  0.252 -0.024 -0.584 -0.285  1.88850\n",
      "1246        1.0  0.043  0.422  0.252 -0.024 -0.584  1.28581\n",
      "1247        1.0 -0.955  0.043  0.422  0.252 -0.024  1.54047\n",
      "1248        1.0  0.130 -0.955  0.043  0.422  0.252  1.42236\n",
      "1249        1.0 -0.298  0.130 -0.955  0.043  0.422  1.38254\n",
      "\n",
      "[1250 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y))\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we are more interested in stock marketing up, we take the second column of y as our response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:          Direction[Up]   No. Observations:                 1250\n",
      "Model:                          Logit   Df Residuals:                     1243\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Sun, 09 Sep 2018   Pseudo R-squ.:                0.002074\n",
      "Time:                        17:53:25   Log-Likelihood:                -863.79\n",
      "converged:                       True   LL-Null:                       -865.59\n",
      "                                        LLR p-value:                    0.7319\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.1260      0.241     -0.523      0.601      -0.598       0.346\n",
      "Lag1          -0.0731      0.050     -1.457      0.145      -0.171       0.025\n",
      "Lag2          -0.0423      0.050     -0.845      0.398      -0.140       0.056\n",
      "Lag3           0.0111      0.050      0.222      0.824      -0.087       0.109\n",
      "Lag4           0.0094      0.050      0.187      0.851      -0.089       0.107\n",
      "Lag5           0.0103      0.050      0.208      0.835      -0.087       0.107\n",
      "Volume         0.1354      0.158      0.855      0.392      -0.175       0.446\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "logit = sm.Logit(y.iloc[:,1], X)  # y.iloc[:,1] is Direction[Up].\n",
    "print(logit.fit().summary())  # Lag1 has the best P value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To extract the parameters directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Intercept   -0.126000\n",
       "Lag1        -0.073074\n",
       "Lag2        -0.042301\n",
       "Lag3         0.011085\n",
       "Lag4         0.009359\n",
       "Lag5         0.010313\n",
       "Volume       0.135441\n",
       "dtype: float64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit().params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To extract the probability of the market going up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0\n",
      "1    1.0\n",
      "2    0.0\n",
      "3    1.0\n",
      "4    1.0\n",
      "5    1.0\n",
      "6    0.0\n",
      "7    1.0\n",
      "8    1.0\n",
      "9    1.0\n",
      "Name: Direction[Up], dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y.iloc[0:10,1]) # ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.50708413, 0.48146788, 0.48113883, ..., 0.5392683 , 0.52611829,\n",
       "       0.51791656])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit().predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.50708413, 0.48146788, 0.48113883, 0.51522236, 0.51078116,\n",
       "       0.50695646, 0.49265087, 0.50922916, 0.51761353, 0.48883778])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit().predict()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up (1) or Down (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_label = pd.DataFrame(np.zeros(shape=(1250,1)), columns = ['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label\n",
      "0       0.0\n",
      "1       0.0\n",
      "2       0.0\n",
      "3       0.0\n",
      "4       0.0\n",
      "5       0.0\n",
      "6       0.0\n",
      "7       0.0\n",
      "8       0.0\n",
      "9       0.0\n",
      "10      0.0\n",
      "11      0.0\n",
      "12      0.0\n",
      "13      0.0\n",
      "14      0.0\n",
      "15      0.0\n",
      "16      0.0\n",
      "17      0.0\n",
      "18      0.0\n",
      "19      0.0\n",
      "20      0.0\n",
      "21      0.0\n",
      "22      0.0\n",
      "23      0.0\n",
      "24      0.0\n",
      "25      0.0\n",
      "26      0.0\n",
      "27      0.0\n",
      "28      0.0\n",
      "29      0.0\n",
      "...     ...\n",
      "1220    0.0\n",
      "1221    0.0\n",
      "1222    0.0\n",
      "1223    0.0\n",
      "1224    0.0\n",
      "1225    0.0\n",
      "1226    0.0\n",
      "1227    0.0\n",
      "1228    0.0\n",
      "1229    0.0\n",
      "1230    0.0\n",
      "1231    0.0\n",
      "1232    0.0\n",
      "1233    0.0\n",
      "1234    0.0\n",
      "1235    0.0\n",
      "1236    0.0\n",
      "1237    0.0\n",
      "1238    0.0\n",
      "1239    0.0\n",
      "1240    0.0\n",
      "1241    0.0\n",
      "1242    0.0\n",
      "1243    0.0\n",
      "1244    0.0\n",
      "1245    0.0\n",
      "1246    0.0\n",
      "1247    0.0\n",
      "1248    0.0\n",
      "1249    0.0\n",
      "\n",
      "[1250 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(predict_label) # dataframe type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit().predict()>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    }
   ],
   "source": [
    "predict_label.iloc[logit.fit().predict()>0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label\n",
      "0       1.0\n",
      "1       0.0\n",
      "2       0.0\n",
      "3       1.0\n",
      "4       1.0\n",
      "5       1.0\n",
      "6       0.0\n",
      "7       1.0\n",
      "8       1.0\n",
      "9       0.0\n",
      "10      0.0\n",
      "11      1.0\n",
      "12      1.0\n",
      "13      0.0\n",
      "14      0.0\n",
      "15      1.0\n",
      "16      1.0\n",
      "17      1.0\n",
      "18      1.0\n",
      "19      0.0\n",
      "20      1.0\n",
      "21      1.0\n",
      "22      1.0\n",
      "23      0.0\n",
      "24      1.0\n",
      "25      1.0\n",
      "26      0.0\n",
      "27      1.0\n",
      "28      1.0\n",
      "29      1.0\n",
      "...     ...\n",
      "1220    1.0\n",
      "1221    1.0\n",
      "1222    1.0\n",
      "1223    1.0\n",
      "1224    1.0\n",
      "1225    1.0\n",
      "1226    0.0\n",
      "1227    1.0\n",
      "1228    1.0\n",
      "1229    1.0\n",
      "1230    1.0\n",
      "1231    1.0\n",
      "1232    1.0\n",
      "1233    1.0\n",
      "1234    1.0\n",
      "1235    1.0\n",
      "1236    1.0\n",
      "1237    1.0\n",
      "1238    1.0\n",
      "1239    1.0\n",
      "1240    1.0\n",
      "1241    1.0\n",
      "1242    1.0\n",
      "1243    1.0\n",
      "1244    1.0\n",
      "1245    1.0\n",
      "1246    1.0\n",
      "1247    1.0\n",
      "1248    1.0\n",
      "1249    1.0\n",
      "\n",
      "[1250 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(predict_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can evalue the TRAINING result by constructing a confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[145, 457],\n",
       "       [141, 507]], dtype=int64)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y.iloc[:,1], predict_label.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  0.0  1.0\n",
      "Actual             \n",
      "0.0        145  457\n",
      "1.0        141  507\n"
     ]
    }
   ],
   "source": [
    "y_actu = pd.Series(y.iloc[:,1], name='Actual')\n",
    "y_pred = pd.Series(predict_label.iloc[:,0], name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "print(df_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. In this case, logistic regression correctly predicted the movement of the market 52.2% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5216"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y.iloc[:,1] == predict_label.iloc[:,0]) # to get accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the held out data. This will yield a more realistic error rate, in the sense that in practice we will be interested in our modelâ€™s performance not on the data that we used to fit the model, but rather on days in the future for which the marketâ€™s movements are unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "Smarket_2005 = Smarket.query('Year >= 2005') # Smarket_2005 = Smarket[Smarket.Year >= 2005]\n",
    "Smarket_train = Smarket.query('Year < 2005') # Smarket_train = Smarket[Smarket.Year < 2005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Year   Lag1   Lag2   Lag3   Lag4   Lag5   Volume  Today Direction\n",
      "998   2005 -0.134  0.008 -0.007  0.715 -0.431  0.78690 -0.812      Down\n",
      "999   2005 -0.812 -0.134  0.008 -0.007  0.715  1.51080 -1.167      Down\n",
      "1000  2005 -1.167 -0.812 -0.134  0.008 -0.007  1.72100 -0.363      Down\n",
      "1001  2005 -0.363 -1.167 -0.812 -0.134  0.008  1.73890  0.351        Up\n",
      "1002  2005  0.351 -0.363 -1.167 -0.812 -0.134  1.56910 -0.143      Down\n",
      "1003  2005 -0.143  0.351 -0.363 -1.167 -0.812  1.47790  0.342        Up\n",
      "1004  2005  0.342 -0.143  0.351 -0.363 -1.167  1.49040 -0.610      Down\n",
      "1005  2005 -0.610  0.342 -0.143  0.351 -0.363  1.48880  0.398        Up\n",
      "1006  2005  0.398 -0.610  0.342 -0.143  0.351  1.56210 -0.863      Down\n",
      "1007  2005 -0.863  0.398 -0.610  0.342 -0.143  1.51030  0.600        Up\n",
      "1008  2005  0.600 -0.863  0.398 -0.610  0.342  1.33540  0.967        Up\n",
      "1009  2005  0.967  0.600 -0.863  0.398 -0.610  1.59680 -0.949      Down\n",
      "1010  2005 -0.949  0.967  0.600 -0.863  0.398  1.49870 -0.778      Down\n",
      "1011  2005 -0.778 -0.949  0.967  0.600 -0.863  1.69200 -0.641      Down\n",
      "1012  2005 -0.641 -0.778 -0.949  0.967  0.600  1.64350 -0.353      Down\n",
      "1013  2005 -0.353 -0.641 -0.778 -0.949  0.967  1.49460  0.400        Up\n",
      "1014  2005  0.400 -0.353 -0.641 -0.778 -0.949  1.61040  0.484        Up\n",
      "1015  2005  0.484  0.400 -0.353 -0.641 -0.778  1.63590  0.041        Up\n",
      "1016  2005  0.041  0.484  0.400 -0.353 -0.641  1.60060 -0.272      Down\n",
      "1017  2005 -0.272  0.041  0.484  0.400 -0.353  1.64180  0.846        Up\n",
      "1018  2005  0.846 -0.272  0.041  0.484  0.400  1.67980  0.689        Up\n",
      "1019  2005  0.689  0.846 -0.272  0.041  0.484  1.68198  0.318        Up\n",
      "1020  2005  0.318  0.689  0.846 -0.272  0.041  1.56174 -0.277      Down\n",
      "1021  2005 -0.277  0.318  0.689  0.846 -0.272  1.55446  1.104        Up\n",
      "1022  2005  1.104 -0.277  0.318  0.689  0.846  1.64816 -0.109      Down\n",
      "1023  2005 -0.109  1.104 -0.277  0.318  0.689  1.34727  0.048        Up\n",
      "1024  2005  0.048 -0.109  1.104 -0.277  0.318  1.41617 -0.858      Down\n",
      "1025  2005 -0.858  0.048 -0.109  1.104 -0.277  1.51104  0.421        Up\n",
      "1026  2005  0.421 -0.858  0.048 -0.109  1.104  1.49167  0.693        Up\n",
      "1027  2005  0.693  0.421 -0.858  0.048 -0.109  1.56230  0.070        Up\n",
      "...    ...    ...    ...    ...    ...    ...      ...    ...       ...\n",
      "1220  2005  0.179 -0.385 -0.078  0.305  0.845  2.12158  0.941        Up\n",
      "1221  2005  0.941  0.179 -0.385 -0.078  0.305  2.29804  0.440        Up\n",
      "1222  2005  0.440  0.941  0.179 -0.385 -0.078  2.45329  0.527        Up\n",
      "1223  2005  0.527  0.440  0.941  0.179 -0.385  2.11735  0.508        Up\n",
      "1224  2005  0.508  0.527  0.440  0.941  0.179  2.29142  0.347        Up\n",
      "1225  2005  0.347  0.508  0.527  0.440  0.941  1.98540  0.209        Up\n",
      "1226  2005  0.209  0.347  0.508  0.527  0.440  0.72494 -0.851      Down\n",
      "1227  2005 -0.851  0.209  0.347  0.508  0.527  2.01690  0.002        Up\n",
      "1228  2005  0.002 -0.851  0.209  0.347  0.508  2.26834 -0.636      Down\n",
      "1229  2005 -0.636  0.002 -0.851  0.209  0.347  2.37469  1.216        Up\n",
      "1230  2005  1.216 -0.636  0.002 -0.851  0.209  2.61483  0.032        Up\n",
      "1231  2005  0.032  1.216 -0.636  0.002 -0.851  2.12558 -0.236      Down\n",
      "1232  2005 -0.236  0.032  1.216 -0.636  0.002  2.32584  0.128        Up\n",
      "1233  2005  0.128 -0.236  0.032  1.216 -0.636  2.11074 -0.501      Down\n",
      "1234  2005 -0.501  0.128 -0.236  0.032  1.216  2.09383 -0.122      Down\n",
      "1235  2005 -0.122 -0.501  0.128 -0.236  0.032  2.17830  0.281        Up\n",
      "1236  2005  0.281 -0.122 -0.501  0.128 -0.236  1.89629  0.084        Up\n",
      "1237  2005  0.084  0.281 -0.122 -0.501  0.128  1.87655  0.555        Up\n",
      "1238  2005  0.555  0.084  0.281 -0.122 -0.501  2.39002  0.419        Up\n",
      "1239  2005  0.419  0.555  0.084  0.281 -0.122  2.14552 -0.141      Down\n",
      "1240  2005 -0.141  0.419  0.555  0.084  0.281  2.18059 -0.285      Down\n",
      "1241  2005 -0.285 -0.141  0.419  0.555  0.084  2.58419 -0.584      Down\n",
      "1242  2005 -0.584 -0.285 -0.141  0.419  0.555  2.20881 -0.024      Down\n",
      "1243  2005 -0.024 -0.584 -0.285 -0.141  0.419  1.99669  0.252        Up\n",
      "1244  2005  0.252 -0.024 -0.584 -0.285 -0.141  2.06517  0.422        Up\n",
      "1245  2005  0.422  0.252 -0.024 -0.584 -0.285  1.88850  0.043        Up\n",
      "1246  2005  0.043  0.422  0.252 -0.024 -0.584  1.28581 -0.955      Down\n",
      "1247  2005 -0.955  0.043  0.422  0.252 -0.024  1.54047  0.130        Up\n",
      "1248  2005  0.130 -0.955  0.043  0.422  0.252  1.42236 -0.298      Down\n",
      "1249  2005 -0.298  0.130 -0.955  0.043  0.422  1.38254 -0.489      Down\n",
      "\n",
      "[252 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(Smarket_2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Year   Lag1   Lag2   Lag3   Lag4   Lag5   Volume  Today Direction\n",
      "0    2001  0.381 -0.192 -2.624 -1.055  5.010  1.19130  0.959        Up\n",
      "1    2001  0.959  0.381 -0.192 -2.624 -1.055  1.29650  1.032        Up\n",
      "2    2001  1.032  0.959  0.381 -0.192 -2.624  1.41120 -0.623      Down\n",
      "3    2001 -0.623  1.032  0.959  0.381 -0.192  1.27600  0.614        Up\n",
      "4    2001  0.614 -0.623  1.032  0.959  0.381  1.20570  0.213        Up\n",
      "5    2001  0.213  0.614 -0.623  1.032  0.959  1.34910  1.392        Up\n",
      "6    2001  1.392  0.213  0.614 -0.623  1.032  1.44500 -0.403      Down\n",
      "7    2001 -0.403  1.392  0.213  0.614 -0.623  1.40780  0.027        Up\n",
      "8    2001  0.027 -0.403  1.392  0.213  0.614  1.16400  1.303        Up\n",
      "9    2001  1.303  0.027 -0.403  1.392  0.213  1.23260  0.287        Up\n",
      "10   2001  0.287  1.303  0.027 -0.403  1.392  1.30900 -0.498      Down\n",
      "11   2001 -0.498  0.287  1.303  0.027 -0.403  1.25800 -0.189      Down\n",
      "12   2001 -0.189 -0.498  0.287  1.303  0.027  1.09800  0.680        Up\n",
      "13   2001  0.680 -0.189 -0.498  0.287  1.303  1.05310  0.701        Up\n",
      "14   2001  0.701  0.680 -0.189 -0.498  0.287  1.14980 -0.562      Down\n",
      "15   2001 -0.562  0.701  0.680 -0.189 -0.498  1.29530  0.546        Up\n",
      "16   2001  0.546 -0.562  0.701  0.680 -0.189  1.11880 -1.747      Down\n",
      "17   2001 -1.747  0.546 -0.562  0.701  0.680  1.04840  0.359        Up\n",
      "18   2001  0.359 -1.747  0.546 -0.562  0.701  1.01300 -0.151      Down\n",
      "19   2001 -0.151  0.359 -1.747  0.546 -0.562  1.05960 -0.841      Down\n",
      "20   2001 -0.841 -0.151  0.359 -1.747  0.546  1.15830 -0.623      Down\n",
      "21   2001 -0.623 -0.841 -0.151  0.359 -1.747  1.10720 -1.334      Down\n",
      "22   2001 -1.334 -0.623 -0.841 -0.151  0.359  1.07550  1.183        Up\n",
      "23   2001  1.183 -1.334 -0.623 -0.841 -0.151  1.03910 -0.865      Down\n",
      "24   2001 -0.865  1.183 -1.334 -0.623 -0.841  1.07520 -0.218      Down\n",
      "25   2001 -0.218 -0.865  1.183 -1.334 -0.623  1.15030  0.812        Up\n",
      "26   2001  0.812 -0.218 -0.865  1.183 -1.334  1.15370 -1.891      Down\n",
      "27   2001 -1.891  0.812 -0.218 -0.865  1.183  1.25720 -1.736      Down\n",
      "28   2001 -1.736 -1.891  0.812 -0.218 -0.865  1.11220 -1.851      Down\n",
      "29   2001 -1.851 -1.736 -1.891  0.812 -0.218  1.20850 -0.195      Down\n",
      "..    ...    ...    ...    ...    ...    ...      ...    ...       ...\n",
      "968  2004  0.554 -0.708 -0.030  0.911  0.909  1.68420  0.136        Up\n",
      "969  2004  0.136  0.554 -0.708 -0.030  0.911  1.45670 -1.116      Down\n",
      "970  2004 -1.116  0.136  0.554 -0.708 -0.030  1.52660  0.590        Up\n",
      "971  2004  0.590 -1.116  0.136  0.554 -0.708  1.39270 -0.025      Down\n",
      "972  2004 -0.025  0.590 -1.116  0.136  0.554  1.42830  0.410        Up\n",
      "973  2004  0.410 -0.025  0.590 -1.116  0.136  1.14960  0.075        Up\n",
      "974  2004  0.075  0.410 -0.025  0.590 -1.116  0.50458 -0.345      Down\n",
      "975  2004 -0.345  0.075  0.410 -0.025  0.590  1.37850 -0.403      Down\n",
      "976  2004 -0.403 -0.345  0.075  0.410 -0.025  1.55350  1.495        Up\n",
      "977  2004  1.495 -0.403 -0.345  0.075  0.410  1.77280 -0.087      Down\n",
      "978  2004 -0.087  1.495 -0.403 -0.345  0.075  1.77490  0.071        Up\n",
      "979  2004  0.071 -0.087  1.495 -0.403 -0.345  1.56670 -0.077      Down\n",
      "980  2004 -0.077  0.071 -0.087  1.495 -0.403  1.35440 -1.107      Down\n",
      "981  2004 -1.107 -0.077  0.071 -0.087  1.495  1.53390  0.488        Up\n",
      "982  2004  0.488 -1.107 -0.077  0.071 -0.087  1.52520  0.544        Up\n",
      "983  2004  0.544  0.488 -1.107 -0.077  0.071  1.62470 -0.104      Down\n",
      "984  2004 -0.104  0.544  0.488 -1.107 -0.077  1.44370  0.899        Up\n",
      "985  2004  0.899 -0.104  0.544  0.488 -1.107  1.43610  0.392        Up\n",
      "986  2004  0.392  0.899 -0.104  0.544  0.488  1.54440  0.194        Up\n",
      "987  2004  0.194  0.392  0.899 -0.104  0.544  1.69580 -0.208      Down\n",
      "988  2004 -0.208  0.194  0.392  0.899 -0.104  1.79390 -0.749      Down\n",
      "989  2004 -0.749 -0.208  0.194  0.392  0.899  2.33500  0.038        Up\n",
      "990  2004  0.038 -0.749 -0.208  0.194  0.392  1.42280  0.904        Up\n",
      "991  2004  0.904  0.038 -0.749 -0.208  0.194  1.48370  0.342        Up\n",
      "992  2004  0.342  0.904  0.038 -0.749 -0.208  1.39080  0.046        Up\n",
      "993  2004  0.046  0.342  0.904  0.038 -0.749  0.95610 -0.431      Down\n",
      "994  2004 -0.431  0.046  0.342  0.904  0.038  0.92200  0.715        Up\n",
      "995  2004  0.715 -0.431  0.046  0.342  0.904  0.98300 -0.007      Down\n",
      "996  2004 -0.007  0.715 -0.431  0.046  0.342  0.92590  0.008        Up\n",
      "997  2004  0.008 -0.007  0.715 -0.431  0.046  0.82980 -0.134      Down\n",
      "\n",
      "[998 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(Smarket_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use the training dataset to build the logistic regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket_train, return_type = 'dataframe')\n",
    "y_test, X_test = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket_2005, return_type = 'dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691936\n",
      "         Iterations 4\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:          Direction[Up]   No. Observations:                  998\n",
      "Model:                          Logit   Df Residuals:                      991\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Sun, 09 Sep 2018   Pseudo R-squ.:                0.001562\n",
      "Time:                        17:53:26   Log-Likelihood:                -690.55\n",
      "converged:                       True   LL-Null:                       -691.63\n",
      "                                        LLR p-value:                    0.9044\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.1912      0.334      0.573      0.567      -0.463       0.845\n",
      "Lag1          -0.0542      0.052     -1.046      0.295      -0.156       0.047\n",
      "Lag2          -0.0458      0.052     -0.884      0.377      -0.147       0.056\n",
      "Lag3           0.0072      0.052      0.139      0.889      -0.094       0.108\n",
      "Lag4           0.0064      0.052      0.125      0.901      -0.095       0.108\n",
      "Lag5          -0.0042      0.051     -0.083      0.934      -0.104       0.096\n",
      "Volume        -0.1163      0.240     -0.485      0.628      -0.586       0.353\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "logit = sm.Logit(y_train.iloc[:,1], X_train)\n",
    "print (logit.fit().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 7)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691936\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[77, 34],\n",
       "       [97, 44]], dtype=int64)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.5\n",
    "# predict_label.iloc[preds > threshold] = 1  # it will cause error\n",
    "predict_label.iloc[preds.values > threshold] = 1  # use preds.values to avoid error\n",
    "confusion_matrix(y_test.iloc[:,1], predict_label.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(preds.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52821955, 0.51566877, 0.52265215, 0.51385435, 0.49833448,\n",
       "       0.50109124, 0.50277035, 0.50956797, 0.5040112 , 0.5106408 ,\n",
       "       0.51011828, 0.48116529, 0.50529498, 0.52363164, 0.51683637,\n",
       "       0.5125333 , 0.49821786, 0.48827676, 0.49601353, 0.50518787,\n",
       "       0.49106893, 0.47897552, 0.49125768, 0.50562365, 0.48891002,\n",
       "       0.49676599, 0.508446  , 0.51682505, 0.50731679, 0.48683967,\n",
       "       0.50074673, 0.50097952, 0.50126923, 0.51237197, 0.51132798,\n",
       "       0.51419056, 0.51202188, 0.48489254, 0.48048287, 0.49749512,\n",
       "       0.50018783, 0.49561147, 0.49970797, 0.48837547, 0.48939833,\n",
       "       0.50885903, 0.51952318, 0.50887292, 0.51091768, 0.50629379,\n",
       "       0.50757365, 0.51889558, 0.50899076, 0.47500344, 0.501955  ,\n",
       "       0.50686645, 0.49207937, 0.49566767, 0.49383793, 0.49170509,\n",
       "       0.47713269, 0.46778418, 0.49536337, 0.49399758, 0.48150925,\n",
       "       0.48687889, 0.48381536, 0.50481923, 0.51397161, 0.48196048,\n",
       "       0.49583069, 0.50824735, 0.50249513, 0.49530876, 0.4707835 ,\n",
       "       0.49341517, 0.47247639, 0.47316413, 0.49242631, 0.49626373,\n",
       "       0.48843384, 0.49578348, 0.47442515, 0.46994494, 0.48062142,\n",
       "       0.46883773, 0.47830147, 0.50422715, 0.48806083, 0.49814402,\n",
       "       0.50230634, 0.49781485, 0.50023765, 0.48541718, 0.46912749,\n",
       "       0.46260007, 0.48177224, 0.49894567, 0.49717326, 0.49371471,\n",
       "       0.50152497, 0.49577802, 0.49803227, 0.5019997 , 0.49151809,\n",
       "       0.48084924, 0.50837546, 0.51140889, 0.49093874, 0.50004706,\n",
       "       0.4910523 , 0.49606301, 0.49976485, 0.49302798, 0.48783991,\n",
       "       0.48980104, 0.46759372, 0.49400476, 0.50262314, 0.4971474 ,\n",
       "       0.50221601, 0.50002333, 0.50531336, 0.48162483, 0.48763474,\n",
       "       0.50007816, 0.50736722, 0.47791094, 0.49372402, 0.50004818,\n",
       "       0.47360156, 0.47050156, 0.4848011 , 0.49408024, 0.48389011,\n",
       "       0.49321042, 0.50835531, 0.48623326, 0.47262198, 0.48960311,\n",
       "       0.49921293, 0.49566042, 0.49296905, 0.48397481, 0.47595941,\n",
       "       0.5012649 , 0.50696088, 0.47677229, 0.47889984, 0.50159614,\n",
       "       0.51166277, 0.50578871, 0.48395853, 0.47850545, 0.48549859,\n",
       "       0.49919101, 0.50566986, 0.50786024, 0.50505516, 0.49472492,\n",
       "       0.50080881, 0.49896204, 0.50156948, 0.5049627 , 0.50614492,\n",
       "       0.50660137, 0.49982856, 0.48958355, 0.46925216, 0.47135814,\n",
       "       0.50354061, 0.48001945, 0.46863181, 0.49503803, 0.48622713,\n",
       "       0.48174396, 0.49890909, 0.50468519, 0.48818721, 0.44294697,\n",
       "       0.48585567, 0.4993329 , 0.49538034, 0.47965473, 0.48322119,\n",
       "       0.48804032, 0.49166692, 0.48493269, 0.47140711, 0.47561173,\n",
       "       0.48987588, 0.49671242, 0.50426983, 0.48709339, 0.48159807,\n",
       "       0.4873724 , 0.4935757 , 0.48572984, 0.48546776, 0.47315179,\n",
       "       0.47373317, 0.49617514, 0.46248313, 0.47277066, 0.49189062,\n",
       "       0.46030755, 0.4609432 , 0.48948392, 0.49942866, 0.46535917,\n",
       "       0.44243083, 0.47468145, 0.46646591, 0.45058518, 0.48358922,\n",
       "       0.48963306, 0.49251726, 0.48507792, 0.46506633, 0.48198601,\n",
       "       0.49230928, 0.48707184, 0.48758656, 0.46513323, 0.45963746,\n",
       "       0.47649324, 0.47044205, 0.48025577, 0.52121466, 0.49920286,\n",
       "       0.49199293, 0.48581824, 0.46110934, 0.47145439, 0.48420234,\n",
       "       0.49011323, 0.49061177, 0.49169945, 0.4898348 , 0.48774809,\n",
       "       0.47073164, 0.47417006, 0.48238257, 0.47974078, 0.49461462,\n",
       "       0.49560127, 0.48328711, 0.48363693, 0.50604836, 0.51665778,\n",
       "       0.51612386, 0.50807231])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "998     0.528220\n",
       "999     0.515669\n",
       "1000    0.522652\n",
       "1001    0.513854\n",
       "1002    0.498334\n",
       "1003    0.501091\n",
       "1004    0.502770\n",
       "1005    0.509568\n",
       "1006    0.504011\n",
       "1007    0.510641\n",
       "1008    0.510118\n",
       "1009    0.481165\n",
       "1010    0.505295\n",
       "1011    0.523632\n",
       "1012    0.516836\n",
       "1013    0.512533\n",
       "1014    0.498218\n",
       "1015    0.488277\n",
       "1016    0.496014\n",
       "1017    0.505188\n",
       "1018    0.491069\n",
       "1019    0.478976\n",
       "1020    0.491258\n",
       "1021    0.505624\n",
       "1022    0.488910\n",
       "1023    0.496766\n",
       "1024    0.508446\n",
       "1025    0.516825\n",
       "1026    0.507317\n",
       "1027    0.486840\n",
       "          ...   \n",
       "1220    0.487587\n",
       "1221    0.465133\n",
       "1222    0.459637\n",
       "1223    0.476493\n",
       "1224    0.470442\n",
       "1225    0.480256\n",
       "1226    0.521215\n",
       "1227    0.499203\n",
       "1228    0.491993\n",
       "1229    0.485818\n",
       "1230    0.461109\n",
       "1231    0.471454\n",
       "1232    0.484202\n",
       "1233    0.490113\n",
       "1234    0.490612\n",
       "1235    0.491699\n",
       "1236    0.489835\n",
       "1237    0.487748\n",
       "1238    0.470732\n",
       "1239    0.474170\n",
       "1240    0.482383\n",
       "1241    0.479741\n",
       "1242    0.494615\n",
       "1243    0.495601\n",
       "1244    0.483287\n",
       "1245    0.483637\n",
       "1246    0.506048\n",
       "1247    0.516658\n",
       "1248    0.516124\n",
       "1249    0.508072\n",
       "Length: 252, dtype: float64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  0.0  1.0\n",
      "Actual             \n",
      "0.0         77   34\n",
      "1.0         97   44\n"
     ]
    }
   ],
   "source": [
    "y_actu = pd.Series(y_test.iloc[:,1], name='Actual')\n",
    "y_pred = pd.Series(predict_label.iloc[:,0], name='Predicted')\n",
    "# reset index: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.reset_index.html\n",
    "df_confusion = pd.crosstab(y_actu.reset_index(drop=True), y_pred)\n",
    "print(df_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "998     0.0\n",
       "999     0.0\n",
       "1000    0.0\n",
       "1001    1.0\n",
       "1002    0.0\n",
       "1003    1.0\n",
       "1004    0.0\n",
       "1005    1.0\n",
       "1006    0.0\n",
       "1007    1.0\n",
       "1008    1.0\n",
       "1009    0.0\n",
       "1010    0.0\n",
       "1011    0.0\n",
       "1012    0.0\n",
       "1013    1.0\n",
       "1014    1.0\n",
       "1015    1.0\n",
       "1016    0.0\n",
       "1017    1.0\n",
       "1018    1.0\n",
       "1019    1.0\n",
       "1020    0.0\n",
       "1021    1.0\n",
       "1022    0.0\n",
       "1023    1.0\n",
       "1024    0.0\n",
       "1025    1.0\n",
       "1026    1.0\n",
       "1027    1.0\n",
       "       ... \n",
       "1220    1.0\n",
       "1221    1.0\n",
       "1222    1.0\n",
       "1223    1.0\n",
       "1224    1.0\n",
       "1225    1.0\n",
       "1226    0.0\n",
       "1227    1.0\n",
       "1228    0.0\n",
       "1229    1.0\n",
       "1230    1.0\n",
       "1231    0.0\n",
       "1232    1.0\n",
       "1233    0.0\n",
       "1234    0.0\n",
       "1235    1.0\n",
       "1236    1.0\n",
       "1237    1.0\n",
       "1238    1.0\n",
       "1239    0.0\n",
       "1240    0.0\n",
       "1241    0.0\n",
       "1242    0.0\n",
       "1243    1.0\n",
       "1244    1.0\n",
       "1245    1.0\n",
       "1246    0.0\n",
       "1247    1.0\n",
       "1248    0.0\n",
       "1249    0.0\n",
       "Name: Actual, Length: 252, dtype: float64"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.0\n",
       "1      1.0\n",
       "2      1.0\n",
       "3      1.0\n",
       "4      0.0\n",
       "5      1.0\n",
       "6      1.0\n",
       "7      1.0\n",
       "8      1.0\n",
       "9      1.0\n",
       "10     1.0\n",
       "11     0.0\n",
       "12     1.0\n",
       "13     1.0\n",
       "14     1.0\n",
       "15     1.0\n",
       "16     0.0\n",
       "17     0.0\n",
       "18     0.0\n",
       "19     1.0\n",
       "20     0.0\n",
       "21     0.0\n",
       "22     0.0\n",
       "23     1.0\n",
       "24     0.0\n",
       "25     0.0\n",
       "26     1.0\n",
       "27     1.0\n",
       "28     1.0\n",
       "29     0.0\n",
       "      ... \n",
       "222    0.0\n",
       "223    0.0\n",
       "224    0.0\n",
       "225    0.0\n",
       "226    0.0\n",
       "227    0.0\n",
       "228    1.0\n",
       "229    0.0\n",
       "230    0.0\n",
       "231    0.0\n",
       "232    0.0\n",
       "233    0.0\n",
       "234    0.0\n",
       "235    0.0\n",
       "236    0.0\n",
       "237    0.0\n",
       "238    0.0\n",
       "239    0.0\n",
       "240    0.0\n",
       "241    0.0\n",
       "242    0.0\n",
       "243    0.0\n",
       "244    0.0\n",
       "245    0.0\n",
       "246    0.0\n",
       "247    0.0\n",
       "248    1.0\n",
       "249    1.0\n",
       "250    1.0\n",
       "251    1.0\n",
       "Name: Predicted, Length: 252, dtype: float64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4801587301587302"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test.iloc[:,1].values==predict_label.iloc[:,0].values) # to get accuracy; use .values since labels are different in two Series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4801587301587302"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test.iloc[:,1].reset_index(drop=True)==predict_label.iloc[:,0]) # alternative expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that we have trained and tested our model on two completely separate data sets: training was performed using only the dates before 2005, and testing was performed using only the dates in 2005. Finally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period. The results are rather disappointing: the test error rate is 1 - 48% = 52 %, which is worse than random guessing! Of course this result is not all that surprising, given that one would not generally expect to be able to use previous daysâ€™ returns to predict future market performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The retrain of the model with Lag1 and Lag2 will be similar to previous steps. Another way to deal with logistics regression is to change the threshold value from 0.5 to others. There is an example below with threshold 0.45. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691936\n",
      "         Iterations 4\n",
      "[[  2 109]\n",
      " [  0 141]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5674603174603174"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Threshold with 0.45\n",
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.45\n",
    "predict_label.iloc[preds.values >threshold] = 1  \n",
    "print(confusion_matrix(y_test.iloc[:,1], predict_label.iloc[:,0]))\n",
    "np.mean(y_test.iloc[:,1].values==predict_label.iloc[:,0].values) # to get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  0.0  1.0\n",
      "Actual             \n",
      "0.0          2  109\n",
      "1.0          0  141\n"
     ]
    }
   ],
   "source": [
    "y_actu = pd.Series(y_test.iloc[:,1], name='Actual')\n",
    "y_pred = pd.Series(predict_label.iloc[:,0], name='Predicted')\n",
    "# reset index: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.reset_index.html\n",
    "df_confusion = pd.crosstab(y_actu.reset_index(drop=True), y_pred)\n",
    "print(df_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692085\n",
      "         Iterations 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5595238095238095"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The retrain of the model with Lag1 and Lag2\n",
    "y_train, X_train = dmatrices('Direction~Lag1+Lag2', Smarket_train, return_type = 'dataframe')\n",
    "y_test, X_test = dmatrices('Direction~Lag1+Lag2', Smarket_2005, return_type = 'dataframe')\n",
    "logit = sm.Logit(y_train.iloc[:,1], X_train)\n",
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.5\n",
    "predict_label.iloc[preds.values >threshold] = 1  \n",
    "np.mean(y_test.iloc[:,1].values==predict_label.iloc[:,0].values) # to get accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.3 Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use sklearn's implementation of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_lda = LDA(n_components=2) # creating a LDA object\n",
    "lda = sklearn_lda.fit(X_train.iloc[:,1:3], y_train.iloc[:,1]) # learning the projection matrix\n",
    "#X_lda = lda.transform(X_train.iloc[:,1:3]) # using the model to project X \n",
    "X_labels = lda.predict(X_train.iloc[:,1:3]) # gives you the predicted label for each sample\n",
    "X_prob = lda.predict_proba(X_train.iloc[:,1:3]) # the probability of each sample to belong to each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_labels=lda.predict(X_test.iloc[:,1:3])\n",
    "X_test_prob = lda.predict_proba(X_test.iloc[:,1:3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49017925, 0.50982075],\n",
       "       [0.4792185 , 0.5207815 ],\n",
       "       [0.46681848, 0.53318152],\n",
       "       [0.47400107, 0.52599893],\n",
       "       [0.49278766, 0.50721234],\n",
       "       [0.49385615, 0.50614385],\n",
       "       [0.49510156, 0.50489844],\n",
       "       [0.4872861 , 0.5127139 ],\n",
       "       [0.49070135, 0.50929865],\n",
       "       [0.48440262, 0.51559738],\n",
       "       [0.49069628, 0.50930372],\n",
       "       [0.51199885, 0.48800115],\n",
       "       [0.48951523, 0.51048477],\n",
       "       [0.47067612, 0.52932388],\n",
       "       [0.47445929, 0.52554071],\n",
       "       [0.47995834, 0.52004166],\n",
       "       [0.49357753, 0.50642247],\n",
       "       [0.50308938, 0.49691062],\n",
       "       [0.49788061, 0.50211939],\n",
       "       [0.48863309, 0.51136691],\n",
       "       [0.50065681, 0.49934319],\n",
       "       [0.51087353, 0.48912647],\n",
       "       [0.50399248, 0.49600752],\n",
       "       [0.49163351, 0.50836649],\n",
       "       [0.50417721, 0.49582279],\n",
       "       [0.50267505, 0.49732495],\n",
       "       [0.49140429, 0.50859571],\n",
       "       [0.48059641, 0.51940359],\n",
       "       [0.48827181, 0.51172819],\n",
       "       [0.50621869, 0.49378131],\n",
       "       [0.50059958, 0.49940042],\n",
       "       [0.49729649, 0.50270351],\n",
       "       [0.49585462, 0.50414538],\n",
       "       [0.48117774, 0.51882226],\n",
       "       [0.48414175, 0.51585825],\n",
       "       [0.47263882, 0.52736118],\n",
       "       [0.48364175, 0.51635825],\n",
       "       [0.50910066, 0.49089934],\n",
       "       [0.51359414, 0.48640586],\n",
       "       [0.49338391, 0.50661609],\n",
       "       [0.49268564, 0.50731436],\n",
       "       [0.4978472 , 0.5021528 ],\n",
       "       [0.49209142, 0.50790858],\n",
       "       [0.50563459, 0.49436541],\n",
       "       [0.50622877, 0.49377123],\n",
       "       [0.48818939, 0.51181061],\n",
       "       [0.47252929, 0.52747071],\n",
       "       [0.48323391, 0.51676609],\n",
       "       [0.48350857, 0.51649143],\n",
       "       [0.49133344, 0.50866656],\n",
       "       [0.48775664, 0.51224336],\n",
       "       [0.47243859, 0.52756141],\n",
       "       [0.48548774, 0.51451226],\n",
       "       [0.49329107, 0.50670893],\n",
       "       [0.48459731, 0.51540269],\n",
       "       [0.47237179, 0.52762821],\n",
       "       [0.48161704, 0.51838296],\n",
       "       [0.49140673, 0.50859327],\n",
       "       [0.4942755 , 0.5057245 ],\n",
       "       [0.48412321, 0.51587679],\n",
       "       [0.50260644, 0.49739356],\n",
       "       [0.50625572, 0.49374428],\n",
       "       [0.48218003, 0.51781997],\n",
       "       [0.48852631, 0.51147369],\n",
       "       [0.50118249, 0.49881751],\n",
       "       [0.50005949, 0.49994051],\n",
       "       [0.50273766, 0.49726234],\n",
       "       [0.48700861, 0.51299139],\n",
       "       [0.48272133, 0.51727867],\n",
       "       [0.49965006, 0.50034994],\n",
       "       [0.4818079 , 0.5181921 ],\n",
       "       [0.4651057 , 0.5348943 ],\n",
       "       [0.45778674, 0.54221326],\n",
       "       [0.47750037, 0.52249963],\n",
       "       [0.50342498, 0.49657502],\n",
       "       [0.48016639, 0.51983361],\n",
       "       [0.50461711, 0.49538289],\n",
       "       [0.50447517, 0.49552483],\n",
       "       [0.4964663 , 0.5035337 ],\n",
       "       [0.48929652, 0.51070348],\n",
       "       [0.48762358, 0.51237642],\n",
       "       [0.48056255, 0.51943745],\n",
       "       [0.4958518 , 0.5041482 ],\n",
       "       [0.51152122, 0.48847878],\n",
       "       [0.49585715, 0.50414285],\n",
       "       [0.50828713, 0.49171287],\n",
       "       [0.50220909, 0.49779091],\n",
       "       [0.48758917, 0.51241083],\n",
       "       [0.49959482, 0.50040518],\n",
       "       [0.48419171, 0.51580829],\n",
       "       [0.48588431, 0.51411569],\n",
       "       [0.48269686, 0.51730314],\n",
       "       [0.47450117, 0.52549883],\n",
       "       [0.50085397, 0.49914603],\n",
       "       [0.51277655, 0.48722345],\n",
       "       [0.51354723, 0.48645277],\n",
       "       [0.50951274, 0.49048726],\n",
       "       [0.49502005, 0.50497995],\n",
       "       [0.49560882, 0.50439118],\n",
       "       [0.49646433, 0.50353567],\n",
       "       [0.48743629, 0.51256371],\n",
       "       [0.49703392, 0.50296608],\n",
       "       [0.50037515, 0.49962485],\n",
       "       [0.48461365, 0.51538635],\n",
       "       [0.49769137, 0.50230863],\n",
       "       [0.50430808, 0.49569192],\n",
       "       [0.48433658, 0.51566342],\n",
       "       [0.48606641, 0.51393359],\n",
       "       [0.49304173, 0.50695827],\n",
       "       [0.48872186, 0.51127814],\n",
       "       [0.49681471, 0.50318529],\n",
       "       [0.49449886, 0.50550114],\n",
       "       [0.49247425, 0.50752575],\n",
       "       [0.49801415, 0.50198585],\n",
       "       [0.49787272, 0.50212728],\n",
       "       [0.49943897, 0.50056103],\n",
       "       [0.50283166, 0.49716834],\n",
       "       [0.49645027, 0.50354973],\n",
       "       [0.48832022, 0.51167978],\n",
       "       [0.48998014, 0.51001986],\n",
       "       [0.4771957 , 0.5228043 ],\n",
       "       [0.46940305, 0.53059695],\n",
       "       [0.48246925, 0.51753075],\n",
       "       [0.50379432, 0.49620568],\n",
       "       [0.50009743, 0.49990257],\n",
       "       [0.48053033, 0.51946967],\n",
       "       [0.48769528, 0.51230472],\n",
       "       [0.50707817, 0.49292183],\n",
       "       [0.49017763, 0.50982237],\n",
       "       [0.48609992, 0.51390008],\n",
       "       [0.51084971, 0.48915029],\n",
       "       [0.51355466, 0.48644534],\n",
       "       [0.50202175, 0.49797825],\n",
       "       [0.49568296, 0.50431704],\n",
       "       [0.49655358, 0.50344642],\n",
       "       [0.49645901, 0.50354099],\n",
       "       [0.48557189, 0.51442811],\n",
       "       [0.4951439 , 0.5048561 ],\n",
       "       [0.50600481, 0.49399519],\n",
       "       [0.48806432, 0.51193568],\n",
       "       [0.49211754, 0.50788246],\n",
       "       [0.49271947, 0.50728053],\n",
       "       [0.49016611, 0.50983389],\n",
       "       [0.5001986 , 0.4998014 ],\n",
       "       [0.50477457, 0.49522543],\n",
       "       [0.48752671, 0.51247329],\n",
       "       [0.48476481, 0.51523519],\n",
       "       [0.50284047, 0.49715953],\n",
       "       [0.50084349, 0.49915651],\n",
       "       [0.48255906, 0.51744094],\n",
       "       [0.47321244, 0.52678756],\n",
       "       [0.47977314, 0.52022686],\n",
       "       [0.49831721, 0.50168279],\n",
       "       [0.49688235, 0.50311765],\n",
       "       [0.49970307, 0.50029693],\n",
       "       [0.49147206, 0.50852794],\n",
       "       [0.48922997, 0.51077003],\n",
       "       [0.47876945, 0.52123055],\n",
       "       [0.47992336, 0.52007664],\n",
       "       [0.49138178, 0.50861822],\n",
       "       [0.49162875, 0.50837125],\n",
       "       [0.49487945, 0.50512055],\n",
       "       [0.48909001, 0.51090999],\n",
       "       [0.47909435, 0.52090565],\n",
       "       [0.4878531 , 0.5121469 ],\n",
       "       [0.48618381, 0.51381619],\n",
       "       [0.49355582, 0.50644418],\n",
       "       [0.49413286, 0.50586714],\n",
       "       [0.50207617, 0.49792383],\n",
       "       [0.50430515, 0.49569485],\n",
       "       [0.48904303, 0.51095697],\n",
       "       [0.50620061, 0.49379939],\n",
       "       [0.50927672, 0.49072328],\n",
       "       [0.48936695, 0.51063305],\n",
       "       [0.49877757, 0.50122243],\n",
       "       [0.4997456 , 0.5002544 ],\n",
       "       [0.48068521, 0.51931479],\n",
       "       [0.47905361, 0.52094639],\n",
       "       [0.48894962, 0.51105038],\n",
       "       [0.50394655, 0.49605345],\n",
       "       [0.49341736, 0.50658264],\n",
       "       [0.4748985 , 0.5251015 ],\n",
       "       [0.4706261 , 0.5293739 ],\n",
       "       [0.48689783, 0.51310217],\n",
       "       [0.49675542, 0.50324458],\n",
       "       [0.49294486, 0.50705514],\n",
       "       [0.49228531, 0.50771469],\n",
       "       [0.493369  , 0.506631  ],\n",
       "       [0.50536007, 0.49463993],\n",
       "       [0.50305521, 0.49694479],\n",
       "       [0.49058366, 0.50941634],\n",
       "       [0.47623902, 0.52376098],\n",
       "       [0.46033919, 0.53966081],\n",
       "       [0.46979321, 0.53020679],\n",
       "       [0.49253001, 0.50746999],\n",
       "       [0.48611431, 0.51388569],\n",
       "       [0.48113758, 0.51886242],\n",
       "       [0.48124736, 0.51875264],\n",
       "       [0.48423833, 0.51576167],\n",
       "       [0.50262179, 0.49737821],\n",
       "       [0.50523122, 0.49476878],\n",
       "       [0.4813184 , 0.5186816 ],\n",
       "       [0.50153968, 0.49846032],\n",
       "       [0.48771613, 0.51228387],\n",
       "       [0.47741706, 0.52258294],\n",
       "       [0.51688267, 0.48311733],\n",
       "       [0.507264  , 0.492736  ],\n",
       "       [0.48335152, 0.51664848],\n",
       "       [0.47267015, 0.52732985],\n",
       "       [0.5032667 , 0.4967333 ],\n",
       "       [0.52023495, 0.47976505],\n",
       "       [0.4950279 , 0.5049721 ],\n",
       "       [0.50187665, 0.49812335],\n",
       "       [0.50891419, 0.49108581],\n",
       "       [0.49689113, 0.50310887],\n",
       "       [0.49515948, 0.50484052],\n",
       "       [0.4895942 , 0.5104058 ],\n",
       "       [0.49046532, 0.50953468],\n",
       "       [0.50553179, 0.49446821],\n",
       "       [0.50554162, 0.49445838],\n",
       "       [0.49424704, 0.50575296],\n",
       "       [0.48574952, 0.51425048],\n",
       "       [0.49016058, 0.50983942],\n",
       "       [0.506973  , 0.493027  ],\n",
       "       [0.50847644, 0.49152356],\n",
       "       [0.50412876, 0.49587124],\n",
       "       [0.50482987, 0.49517013],\n",
       "       [0.50238787, 0.49761213],\n",
       "       [0.49869029, 0.50130971],\n",
       "       [0.48247575, 0.51752425],\n",
       "       [0.48254694, 0.51745306],\n",
       "       [0.48316002, 0.51683998],\n",
       "       [0.50174966, 0.49825034],\n",
       "       [0.50587076, 0.49412924],\n",
       "       [0.48903208, 0.51096792],\n",
       "       [0.49110524, 0.50889476],\n",
       "       [0.48642499, 0.51357501],\n",
       "       [0.48470615, 0.51529385],\n",
       "       [0.49448897, 0.50551103],\n",
       "       [0.49622614, 0.50377386],\n",
       "       [0.50057022, 0.49942978],\n",
       "       [0.5039068 , 0.4960932 ],\n",
       "       [0.49463764, 0.50536236],\n",
       "       [0.48643657, 0.51356343],\n",
       "       [0.4807022 , 0.5192978 ],\n",
       "       [0.48514389, 0.51485611],\n",
       "       [0.49517341, 0.50482659],\n",
       "       [0.50058931, 0.49941069],\n",
       "       [0.497221  , 0.502779  ],\n",
       "       [0.4791988 , 0.5208012 ],\n",
       "       [0.48316733, 0.51683267],\n",
       "       [0.4892591 , 0.5107409 ]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the accuracy of the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5595238095238095"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test.iloc[:,1]==X_test_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> (252,)\n",
      "<class 'numpy.ndarray'> (252,)\n"
     ]
    }
   ],
   "source": [
    "print(type(y_test.iloc[:,1]), y_test.iloc[:,1].shape)\n",
    "print(type(X_test_labels), X_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's change the threshod a bit to see whether we can improve the accuracy. The 2nd column of X_test_prob is the probability belongs to UP group. The default value is 0.5, let us first check that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5595238095238095"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.5 \n",
    "np.mean(y_test.iloc[:,1]==(X_test_prob[:,1]>=threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5634920634920635"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.48\n",
    "np.mean(y_test.iloc[:,1]==(X_test_prob[:,1]>=threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.4 Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QDA and LDA have minor difference in their parameter set-up and function names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "# http://scikit-learn.org/0.16/modules/generated/sklearn.qda.QDA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5992063492063492\n"
     ]
    }
   ],
   "source": [
    "sklearn_qda = QDA(priors=None,store_covariance=True) # creating a QDA object\n",
    "qda = sklearn_qda.fit(X_train.iloc[:,1:3], y_train.iloc[:,1]) # learning the projection matrix\n",
    "X_labels = qda.predict(X_train.iloc[:,1:3]) # gives you the predicted label for each sample\n",
    "X_prob = qda.predict_proba(X_train.iloc[:,1:3]) # the probability of each sample to belong to each class\n",
    "\n",
    "X_test_labels=qda.predict(X_test.iloc[:,1:3])\n",
    "X_test_prob = qda.predict_proba(X_test.iloc[:,1:3]) \n",
    "\n",
    "print (np.mean(y_test.iloc[:,1]==X_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, use dir() to explore all the information stored in lda and qda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_decision_function',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " 'classes_',\n",
       " 'covariance_',\n",
       " 'covariances_',\n",
       " 'decision_function',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'means_',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'priors',\n",
       " 'priors_',\n",
       " 'reg_param',\n",
       " 'rotations_',\n",
       " 'scalings_',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'store_covariance',\n",
       " 'store_covariances',\n",
       " 'tol']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]]\n",
      "[array([[ 1.50662277, -0.03924806],\n",
      "       [-0.03924806,  1.53559498]]), array([[ 1.51700576, -0.02787349],\n",
      "       [-0.02787349,  1.49026815]])]\n"
     ]
    }
   ],
   "source": [
    "print (qda.means_)  # mean vector for each class\n",
    "print (qda.covariance_) # covariance for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.5 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5317460317460317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_estimator_type',\n",
       " '_fit',\n",
       " '_fit_X',\n",
       " '_fit_method',\n",
       " '_get_param_names',\n",
       " '_init_params',\n",
       " '_pairwise',\n",
       " '_tree',\n",
       " '_y',\n",
       " 'algorithm',\n",
       " 'classes_',\n",
       " 'effective_metric_',\n",
       " 'effective_metric_params_',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'kneighbors',\n",
       " 'kneighbors_graph',\n",
       " 'leaf_size',\n",
       " 'metric',\n",
       " 'metric_params',\n",
       " 'n_jobs',\n",
       " 'n_neighbors',\n",
       " 'outputs_2d_',\n",
       " 'p',\n",
       " 'predict',\n",
       " 'predict_proba',\n",
       " 'radius',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'weights']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNN(n_neighbors= 3) # use n_neighbors to change the # of tune the performance of KNN\n",
    "KNN_fit = neigh.fit(X_train.iloc[:,1:3], y_train.iloc[:,1]) #learning the projection matrix\n",
    "X_test_labels=KNN_fit.predict(X_test.iloc[:,1:3])\n",
    "X_test_prob = KNN_fit.predict_proba(X_test.iloc[:,1:3]) \n",
    "\n",
    "print (np.mean(y_test.iloc[:,1]==X_test_labels))\n",
    "\n",
    "dir(neigh) # use dir command to check what KNN offers"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
